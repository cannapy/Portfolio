{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "PoRTyUc94S1a",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Faster R-CNN: A Classic Two-Stage Anchor-Based Object Detector\n",
        "\n",
        "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules - Region Proposal Networks (RPN) and Fast R-CNN.\n",
        "We will train it to detect a set of object classes and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "LfBk3NtRgqaV",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "RrAX9FOLpr9k",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch, random\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import models\n",
        "from torchvision.models import feature_extraction\n",
        "\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "TexqwuYLoHRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper code\n",
        "\n",
        "## No need to make any code changes here. Just download the dataset in a folder and specify the path, as mentioned later. \n",
        "## If facing issue with downloading data, you can download the data manually and change the paths accordingly wherever needed in this assignment as you did for previous assignments.\n",
        "\n",
        "TensorDict = Dict[str, torch.Tensor]\n",
        "\n",
        "class VOC2007DetectionTiny(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A tiny version of PASCAL VOC 2007 Detection dataset that includes images and\n",
        "    annotations with small images and no difficult boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_dir: str,\n",
        "        split: str = \"train\",\n",
        "        download: bool = False,\n",
        "        image_size: int = 224,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            download: Whether to download PASCAL VOC 2007 to `dataset_dir`.\n",
        "            image_size: Size of imges in the batch. The shorter edge of images\n",
        "                will be resized to this size, followed by a center crop. For\n",
        "                val, center crop will not be taken to capture all detections.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Attempt to download the dataset from Justin's server:\n",
        "        if download:\n",
        "            self._attempt_download(dataset_dir)\n",
        "\n",
        "        # fmt: off\n",
        "        voc_classes = [\n",
        "            \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
        "            \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\",\n",
        "            \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
        "            \"sofa\", \"train\", \"tvmonitor\"\n",
        "        ]\n",
        "        # fmt: on\n",
        "\n",
        "        # Make a (class to ID) and inverse (ID to class) mapping.\n",
        "        self.class_to_idx = {\n",
        "            _class: _idx for _idx, _class in enumerate(voc_classes)\n",
        "        }\n",
        "        self.idx_to_class = {\n",
        "            _idx: _class for _idx, _class in enumerate(voc_classes)\n",
        "        }\n",
        "\n",
        "        # Load instances from JSON file:\n",
        "        self.instances = json.load(\n",
        "            open(os.path.join(dataset_dir, f\"voc07_{split}.json\"))\n",
        "        )\n",
        "        self.dataset_dir = dataset_dir\n",
        "\n",
        "        # Define a transformation function for image: Resize the shorter image\n",
        "        # edge then take a center crop (optional) and normalize.\n",
        "        _transforms = [\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "            ),\n",
        "        ]\n",
        "        self.image_transform = transforms.Compose(_transforms)\n",
        "\n",
        "    @staticmethod\n",
        "    def _attempt_download(dataset_dir: str):\n",
        "        \"\"\"\n",
        "        Try to download VOC dataset and save it to `dataset_dir`.\n",
        "        \"\"\"\n",
        "        import wget\n",
        "\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "        # fmt: off\n",
        "        wget.download(\n",
        "            \"https://web.eecs.umich.edu/~justincj/data/VOCtrainval_06-Nov-2007.tar\",\n",
        "            out=dataset_dir,\n",
        "        )\n",
        "        wget.download(\n",
        "            \"https://web.eecs.umich.edu/~justincj/data/voc07_train.json\",\n",
        "            out=dataset_dir,\n",
        "        )\n",
        "        wget.download(\n",
        "            \"https://web.eecs.umich.edu/~justincj/data/voc07_val.json\",\n",
        "            out=dataset_dir,\n",
        "        )\n",
        "        # fmt: on\n",
        "\n",
        "        # Extract TAR file:\n",
        "        import tarfile\n",
        "\n",
        "        voc_tar = tarfile.open(\n",
        "            os.path.join(dataset_dir, \"VOCtrainval_06-Nov-2007.tar\")\n",
        "        )\n",
        "        voc_tar.extractall(dataset_dir)\n",
        "        voc_tar.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # PIL image and dictionary of annotations.\n",
        "        image_path, ann = self.instances[index]\n",
        "        # TODO: Remove this after the JSON files are fixed on Justin's server:\n",
        "        image_path = image_path.replace(\"./here/\", \"\")\n",
        "        image_path = os.path.join(self.dataset_dir, image_path)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Collect a list of GT boxes: (N, 4), and GT classes: (N, )\n",
        "        gt_boxes = torch.tensor([inst[\"xyxy\"] for inst in ann])\n",
        "        gt_classes = torch.Tensor([self.class_to_idx[inst[\"name\"]] for inst in ann])\n",
        "        gt_classes = gt_classes.unsqueeze(1)  # (N, 1)\n",
        "\n",
        "        # Record original image size before transforming.\n",
        "        original_width, original_height = image.size\n",
        "\n",
        "        # Normalize bounding box co-ordinates to bring them in [0, 1]. This is\n",
        "        # temporary, simply to ease the transformation logic.\n",
        "        normalize_tens = torch.tensor(\n",
        "            [original_width, original_height, original_width, original_height]\n",
        "        )\n",
        "        gt_boxes /= normalize_tens[None, :]\n",
        "\n",
        "        # Transform input image to CHW tensor.\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        # WARN: Even dimensions should be even numbers else it messes up\n",
        "        # upsampling in FPN.\n",
        "\n",
        "        # Apply image resizing transformation to bounding boxes.\n",
        "        if self.image_size is not None:\n",
        "            if original_height >= original_width:\n",
        "                new_width = self.image_size\n",
        "                new_height = original_height * self.image_size / original_width\n",
        "            else:\n",
        "                new_height = self.image_size\n",
        "                new_width = original_width * self.image_size / original_height\n",
        "\n",
        "            _x1 = (new_width - self.image_size) // 2\n",
        "            _y1 = (new_height - self.image_size) // 2\n",
        "\n",
        "            # Un-normalize bounding box co-ordinates and shift due to center crop.\n",
        "            # Clamp to (0, image size).\n",
        "            gt_boxes[:, 0] = torch.clamp(gt_boxes[:, 0] * new_width - _x1, min=0)\n",
        "            gt_boxes[:, 1] = torch.clamp(gt_boxes[:, 1] * new_height - _y1, min=0)\n",
        "            gt_boxes[:, 2] = torch.clamp(\n",
        "                gt_boxes[:, 2] * new_width - _x1, max=self.image_size\n",
        "            )\n",
        "            gt_boxes[:, 3] = torch.clamp(\n",
        "                gt_boxes[:, 3] * new_height - _y1, max=self.image_size\n",
        "            )\n",
        "\n",
        "        # Concatenate GT classes with GT boxes; shape: (N, 5)\n",
        "        gt_boxes = torch.cat([gt_boxes, gt_classes], dim=1)\n",
        "\n",
        "        # Center cropping may completely exclude certain boxes that were close\n",
        "        # to image boundaries. Set them to -1\n",
        "        invalid = (gt_boxes[:, 0] > gt_boxes[:, 2]) | (\n",
        "            gt_boxes[:, 1] > gt_boxes[:, 3]\n",
        "        )\n",
        "        gt_boxes[invalid] = -1\n",
        "\n",
        "        # Pad to max 40 boxes, that's enough for VOC.\n",
        "        gt_boxes = torch.cat(\n",
        "            [gt_boxes, torch.zeros(40 - len(gt_boxes), 5).fill_(-1.0)]\n",
        "        )\n",
        "        # Return image path because it is needed for evaluation.\n",
        "        return image_path, image, gt_boxes\n",
        "\n",
        "\n",
        "\n",
        "def detection_visualizer(img, idx_to_class, bbox=None, pred=None, points=None):\n",
        "    \"\"\"\n",
        "    Data visualizer on the original image. Support both GT\n",
        "    box input and proposal input.\n",
        "\n",
        "    Input:\n",
        "    - img: PIL Image input\n",
        "    - idx_to_class: Mapping from the index (0-19) to the class name\n",
        "    - bbox: GT bbox (in red, optional), a tensor of shape Nx5, where N is\n",
        "            the number of GT boxes, 5 indicates\n",
        "            (x_tl, y_tl, x_br, y_br, class)\n",
        "    - pred: Predicted bbox (in green, optional),\n",
        "            a tensor of shape N'x6, where N' is the number\n",
        "            of predicted boxes, 6 indicates\n",
        "            (x_tl, y_tl, x_br, y_br, class, object confidence score)\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert image to HWC if it is passed as a Tensor (0-1, CHW).\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = (img * 255).permute(1, 2, 0)\n",
        "\n",
        "    img_copy = np.array(img).astype(\"uint8\")\n",
        "    _, ax = plt.subplots(frameon=False)\n",
        "\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(img_copy)\n",
        "\n",
        "    # fmt: off\n",
        "    if points is not None:\n",
        "        points_x = [t[0] for t in points]\n",
        "        points_y = [t[1] for t in points]\n",
        "        ax.scatter(points_x, points_y, color=\"yellow\", s=24)\n",
        "\n",
        "    if bbox is not None:\n",
        "        for single_bbox in bbox:\n",
        "            x0, y0, x1, y1 = single_bbox[:4]\n",
        "            width = x1 - x0\n",
        "            height = y1 - y0\n",
        "\n",
        "            ax.add_patch(\n",
        "                mpl.patches.Rectangle(\n",
        "                    (x0, y0), width, height, fill=False, edgecolor=(1.0, 0, 0),\n",
        "                    linewidth=4, linestyle=\"solid\",\n",
        "                )\n",
        "            )\n",
        "            if len(single_bbox) > 4:  # if class info provided\n",
        "                obj_cls = idx_to_class[single_bbox[4].item()]\n",
        "                ax.text(\n",
        "                    x0, y0, obj_cls, size=18, family=\"sans-serif\",\n",
        "                    bbox={\n",
        "                        \"facecolor\": \"black\", \"alpha\": 0.8,\n",
        "                        \"pad\": 0.7, \"edgecolor\": \"none\"\n",
        "                    },\n",
        "                    verticalalignment=\"top\",\n",
        "                    color=(1, 1, 1),\n",
        "                    zorder=10,\n",
        "                )\n",
        "\n",
        "    if pred is not None:\n",
        "        for single_bbox in pred:\n",
        "            x0, y0, x1, y1 = single_bbox[:4]\n",
        "            width = x1 - x0\n",
        "            height = y1 - y0\n",
        "\n",
        "            ax.add_patch(\n",
        "                mpl.patches.Rectangle(\n",
        "                    (x0, y0), width, height, fill=False, edgecolor=(0, 1.0, 0),\n",
        "                    linewidth=4, linestyle=\"solid\",\n",
        "                )\n",
        "            )\n",
        "            if len(single_bbox) > 4:  # if class info provided\n",
        "                obj_cls = idx_to_class[single_bbox[4].item()]\n",
        "                conf_score = single_bbox[5].item()\n",
        "                ax.text(\n",
        "                    x0, y0 + 15, f\"{obj_cls}, {conf_score:.2f}\",\n",
        "                    size=18, family=\"sans-serif\",\n",
        "                    bbox={\n",
        "                        \"facecolor\": \"black\", \"alpha\": 0.8,\n",
        "                        \"pad\": 0.7, \"edgecolor\": \"none\"\n",
        "                    },\n",
        "                    verticalalignment=\"top\",\n",
        "                    color=(1, 1, 1),\n",
        "                    zorder=10,\n",
        "                )\n",
        "    # fmt: on\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def rcnn_match_anchors_to_gt(\n",
        "    anchor_boxes: torch.Tensor,\n",
        "    gt_boxes: torch.Tensor,\n",
        "    iou_thresholds: Tuple[float, float],\n",
        ") -> TensorDict:\n",
        "    \"\"\"\n",
        "    Match anchor boxes (or RPN proposals) with a set of GT boxes. Anchors having\n",
        "    high IoU with any GT box are assigned \"foreground\" and matched with that box\n",
        "    or vice-versa.\n",
        "\n",
        "    NOTE: This function is NOT BATCHED. Call separately for GT boxes per image.\n",
        "\n",
        "    Args:\n",
        "        anchor_boxes: Anchor boxes (or RPN proposals). Dictionary of three keys\n",
        "            a combined tensor of some shape `(N, 4)` where `N` are total anchors\n",
        "            from all FPN levels, or a set of RPN proposals.\n",
        "        gt_boxes: GT boxes of a single image, a batch of `(M, 5)` boxes with\n",
        "            absolute co-ordinates and class ID `(x1, y1, x2, y2, C)`. In this\n",
        "            codebase, this tensor is directly served by the dataloader.\n",
        "        iou_thresholds: Tuple of (low, high) IoU thresholds, both in [0, 1]\n",
        "            giving thresholds to assign foreground/background anchors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter empty GT boxes:\n",
        "    gt_boxes = gt_boxes[gt_boxes[:, 4] != -1]\n",
        "\n",
        "    # If no GT boxes are available, match all anchors to background and return.\n",
        "    if len(gt_boxes) == 0:\n",
        "        fake_boxes = torch.zeros_like(anchor_boxes) - 1\n",
        "        fake_class = torch.zeros_like(anchor_boxes[:, [0]]) - 1\n",
        "        return torch.cat([fake_boxes, fake_class], dim=1)\n",
        "\n",
        "    # Match matrix => pairwise IoU of anchors (rows) and GT boxes (columns).\n",
        "    # STUDENTS: This matching depends on your IoU implementation.\n",
        "    match_matrix = iou(anchor_boxes, gt_boxes[:, :4])\n",
        "\n",
        "    # Find matched ground-truth instance per anchor:\n",
        "    match_quality, matched_idxs = match_matrix.max(dim=1)\n",
        "    matched_gt_boxes = gt_boxes[matched_idxs]\n",
        "\n",
        "    # Set boxes with low IoU threshold to background (-1).\n",
        "    matched_gt_boxes[match_quality <= iou_thresholds[0]] = -1\n",
        "\n",
        "    # Set remaining boxes to neutral (-1e8).\n",
        "    neutral_idxs = (match_quality > iou_thresholds[0]) & (\n",
        "        match_quality < iou_thresholds[1]\n",
        "    )\n",
        "    matched_gt_boxes[neutral_idxs, :] = -1e8\n",
        "    return matched_gt_boxes\n",
        "\n",
        "\n",
        "\n",
        "def inference_with_detector(\n",
        "    detector,\n",
        "    test_loader,\n",
        "    idx_to_class,\n",
        "    score_thresh: float,\n",
        "    nms_thresh: float,\n",
        "    output_dir: Optional[str] = None,\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    device:str = \"cpu\",\n",
        "):\n",
        "\n",
        "    # ship model to GPU\n",
        "    detector.to(dtype=dtype, device=device)\n",
        "\n",
        "    detector.eval()\n",
        "    start_t = time.time()\n",
        "\n",
        "    # Define an \"inverse\" transform for the image that un-normalizes by ImageNet\n",
        "    # color. Without this, the images will NOT be visually understandable.\n",
        "    inverse_norm = transforms.Compose(\n",
        "        [\n",
        "            transforms.Normalize(\n",
        "                mean=[0.0, 0.0, 0.0], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
        "            ),\n",
        "            transforms.Normalize(\n",
        "                mean=[-0.485, -0.456, -0.406], std=[1.0, 1.0, 1.0]\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if output_dir is not None:\n",
        "        det_dir = \"mAP/input/detection-results\"\n",
        "        gt_dir = \"mAP/input/ground-truth\"\n",
        "        if os.path.exists(det_dir):\n",
        "            shutil.rmtree(det_dir)\n",
        "        os.mkdir(det_dir)\n",
        "        if os.path.exists(gt_dir):\n",
        "            shutil.rmtree(gt_dir)\n",
        "        os.mkdir(gt_dir)\n",
        "\n",
        "    for iter_num, test_batch in enumerate(test_loader):\n",
        "        image_paths, images, gt_boxes = test_batch\n",
        "        images = images.to(dtype=dtype, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if score_thresh is not None and nms_thresh is not None:\n",
        "                # shapes: (num_preds, 4) (num_preds, ) (num_preds, )\n",
        "                pred_boxes, pred_classes, pred_scores = detector(\n",
        "                    images,\n",
        "                    test_score_thresh=score_thresh,\n",
        "                    test_nms_thresh=nms_thresh,\n",
        "                )\n",
        "\n",
        "        # Skip current iteration if no predictions were found.\n",
        "        if pred_boxes.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Remove padding (-1) and batch dimension from predicted / GT boxes\n",
        "        # and transfer to CPU. Indexing `[0]` here removes batch dimension:\n",
        "        gt_boxes = gt_boxes[0]\n",
        "        valid_gt = gt_boxes[:, 4] != -1\n",
        "        gt_boxes = gt_boxes[valid_gt].cpu()\n",
        "\n",
        "        valid_pred = pred_classes != -1\n",
        "        pred_boxes = pred_boxes[valid_pred].cpu()\n",
        "        pred_classes = pred_classes[valid_pred].cpu()\n",
        "        pred_scores = pred_scores[valid_pred].cpu()\n",
        "\n",
        "        image_path = image_paths[0]\n",
        "        # Un-normalize image tensor for visualization.\n",
        "        image = inverse_norm(images[0]).cpu()\n",
        "\n",
        "        # Combine predicted classes and scores into boxes for evaluation\n",
        "        # and visualization.\n",
        "        pred_boxes = torch.cat(\n",
        "            [pred_boxes, pred_classes.unsqueeze(1), pred_scores.unsqueeze(1)], dim=1\n",
        "        )\n",
        "\n",
        "        # write results to file for evaluation (use mAP API https://github.com/Cartucho/mAP for now...)\n",
        "        if output_dir is not None:\n",
        "            file_name = os.path.basename(image_path).replace(\".jpg\", \".txt\")\n",
        "            with open(os.path.join(det_dir, file_name), \"w\") as f_det, open(\n",
        "                os.path.join(gt_dir, file_name), \"w\"\n",
        "            ) as f_gt:\n",
        "                for b in gt_boxes:\n",
        "                    f_gt.write(\n",
        "                        f\"{idx_to_class[b[4].item()]} {b[0]:.2f} {b[1]:.2f} {b[2]:.2f} {b[3]:.2f}\\n\"\n",
        "                    )\n",
        "                for b in pred_boxes:\n",
        "                    f_det.write(\n",
        "                        f\"{idx_to_class[b[4].item()]} {b[5]:.6f} {b[0]:.2f} {b[1]:.2f} {b[2]:.2f} {b[3]:.2f}\\n\"\n",
        "                    )\n",
        "        else:\n",
        "            detection_visualizer(\n",
        "                image, idx_to_class, gt_boxes, pred_boxes\n",
        "            )\n",
        "\n",
        "    end_t = time.time()\n",
        "    print(f\"Total inference time: {end_t-start_t:.1f}s\")\n",
        "\n",
        "\n",
        "\n",
        "def detection_visualizer(img, idx_to_class, bbox=None, pred=None, points=None):\n",
        "    \"\"\"\n",
        "    Data visualizer on the original image. Support both GT\n",
        "    box input and proposal input.\n",
        "\n",
        "    Input:\n",
        "    - img: PIL Image input\n",
        "    - idx_to_class: Mapping from the index (0-19) to the class name\n",
        "    - bbox: GT bbox (in red, optional), a tensor of shape Nx5, where N is\n",
        "            the number of GT boxes, 5 indicates\n",
        "            (x_tl, y_tl, x_br, y_br, class)\n",
        "    - pred: Predicted bbox (in green, optional),\n",
        "            a tensor of shape N'x6, where N' is the number\n",
        "            of predicted boxes, 6 indicates\n",
        "            (x_tl, y_tl, x_br, y_br, class, object confidence score)\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert image to HWC if it is passed as a Tensor (0-1, CHW).\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = (img * 255).permute(1, 2, 0)\n",
        "\n",
        "    img_copy = np.array(img).astype(\"uint8\")\n",
        "    _, ax = plt.subplots(frameon=False)\n",
        "\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(img_copy)\n",
        "\n",
        "    # fmt: off\n",
        "    if points is not None:\n",
        "        points_x = [t[0] for t in points]\n",
        "        points_y = [t[1] for t in points]\n",
        "        ax.scatter(points_x, points_y, color=\"yellow\", s=24)\n",
        "\n",
        "    if bbox is not None:\n",
        "        for single_bbox in bbox:\n",
        "            x0, y0, x1, y1 = single_bbox[:4]\n",
        "            width = x1 - x0\n",
        "            height = y1 - y0\n",
        "\n",
        "            ax.add_patch(\n",
        "                mpl.patches.Rectangle(\n",
        "                    (x0, y0), width, height, fill=False, edgecolor=(1.0, 0, 0),\n",
        "                    linewidth=4, linestyle=\"solid\",\n",
        "                )\n",
        "            )\n",
        "            if len(single_bbox) > 4:  # if class info provided\n",
        "                obj_cls = idx_to_class[single_bbox[4].item()]\n",
        "                ax.text(\n",
        "                    x0, y0, obj_cls, size=18, family=\"sans-serif\",\n",
        "                    bbox={\n",
        "                        \"facecolor\": \"black\", \"alpha\": 0.8,\n",
        "                        \"pad\": 0.7, \"edgecolor\": \"none\"\n",
        "                    },\n",
        "                    verticalalignment=\"top\",\n",
        "                    color=(1, 1, 1),\n",
        "                    zorder=10,\n",
        "                )\n",
        "\n",
        "    if pred is not None:\n",
        "        for single_bbox in pred:\n",
        "            x0, y0, x1, y1 = single_bbox[:4]\n",
        "            width = x1 - x0\n",
        "            height = y1 - y0\n",
        "\n",
        "            ax.add_patch(\n",
        "                mpl.patches.Rectangle(\n",
        "                    (x0, y0), width, height, fill=False, edgecolor=(0, 1.0, 0),\n",
        "                    linewidth=4, linestyle=\"solid\",\n",
        "                )\n",
        "            )\n",
        "            if len(single_bbox) > 4:  # if class info provided\n",
        "                obj_cls = idx_to_class[single_bbox[4].item()]\n",
        "                conf_score = single_bbox[5].item()\n",
        "                ax.text(\n",
        "                    x0, y0 + 15, f\"{obj_cls}, {conf_score:.2f}\",\n",
        "                    size=18, family=\"sans-serif\",\n",
        "                    bbox={\n",
        "                        \"facecolor\": \"black\", \"alpha\": 0.8,\n",
        "                        \"pad\": 0.7, \"edgecolor\": \"none\"\n",
        "                    },\n",
        "                    verticalalignment=\"top\",\n",
        "                    color=(1, 1, 1),\n",
        "                    zorder=10,\n",
        "                )\n",
        "    # fmt: on\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def class_spec_nms(\n",
        "    boxes: torch.Tensor,\n",
        "    scores: torch.Tensor,\n",
        "    class_ids: torch.Tensor,\n",
        "    iou_threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Wrap `nms` to make it class-specific. Pass class IDs as `class_ids`.\n",
        "    STUDENT: This depends on your `nms` implementation.\n",
        "\n",
        "    Returns:\n",
        "        keep: torch.long tensor with the indices of the elements that have been\n",
        "            kept by NMS, sorted in decreasing order of scores;\n",
        "            of shape [num_kept_boxes]\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n",
        "    max_coordinate = boxes.max()\n",
        "    offsets = class_ids.to(boxes) * (max_coordinate + torch.tensor(1).to(boxes))\n",
        "    boxes_for_nms = boxes + offsets[:, None]\n",
        "    keep = nms(boxes_for_nms, scores, iou_threshold)\n",
        "    return keep\n",
        "\n",
        "\n",
        "def mix_gt_with_proposals(\n",
        "    proposals_per_fpn_level: Dict[str, List[torch.Tensor]], gt_boxes: torch.Tensor\n",
        "):\n",
        "    \"\"\"\n",
        "    At start of training, RPN proposals may be low quality. It's possible that\n",
        "    very few of these have high IoU with GT boxes. This may stall or de-stabilize\n",
        "    training of second stage. This function mixes GT boxes with RPN proposals to\n",
        "    improve training. Different GT boxes are mixed with proposals from different\n",
        "    FPN levels according to assignment rule of FPN paper.\n",
        "\n",
        "    Args:\n",
        "        proposals_per_fpn_level: Dict of proposals per FPN level, per image in\n",
        "            batch. These are same as outputs from `RPN.forward()` method.\n",
        "        gt_boxes: Tensor of shape `(B, M, 4 or 5)` giving GT boxes per image in\n",
        "            batch (with or without GT class label, doesn't matter).\n",
        "\n",
        "    Returns:\n",
        "        proposals_per_fpn_level: Same as input, but with GT boxes mixed in them.\n",
        "    \"\"\"\n",
        "\n",
        "    # Mix ground-truth boxes for every example, per FPN level. There's no direct\n",
        "    # way to vectorize this.\n",
        "    for _idx, _gtb in enumerate(gt_boxes):\n",
        "\n",
        "        # Filter empty GT boxes:\n",
        "        _gtb = _gtb[_gtb[:, 4] != -1]\n",
        "        if len(_gtb) == 0:\n",
        "            continue\n",
        "\n",
        "        # Compute FPN level assignments for each GT box. This follows Equation (1)\n",
        "        # of FPN paper (k0 = 5). `level_assn` has `(M, )` integers, one of {3,4,5}\n",
        "        _gt_area = (_gtb[:, 2] - _gtb[:, 0]) * (_gtb[:, 3] - _gtb[:, 1])\n",
        "        level_assn = torch.floor(5 + torch.log2(torch.sqrt(_gt_area) / 224))\n",
        "        level_assn = torch.clamp(level_assn, min=3, max=5).to(torch.int64)\n",
        "\n",
        "        for level_name, _props in proposals_per_fpn_level.items():\n",
        "            _prop = _props[_idx]\n",
        "\n",
        "            # Get GT boxes of this image that match level scale, and append them\n",
        "            # to proposals.\n",
        "            _gt_boxes_fpn_subset = _gtb[level_assn == int(level_name[1])]\n",
        "            if len(_gt_boxes_fpn_subset) > 0:\n",
        "                proposals_per_fpn_level[level_name][_idx] = torch.cat(\n",
        "                    # Remove class label since proposals don't have it:\n",
        "                    [_prop, _gt_boxes_fpn_subset[:, :4]],\n",
        "                    dim=0,\n",
        "                )\n",
        "\n",
        "    return proposals_per_fpn_level\n",
        "    \n",
        "\n",
        "def sample_rpn_training(\n",
        "    gt_boxes: torch.Tensor, num_samples: int, fg_fraction: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Return `num_samples` (or fewer, if not enough found) random pairs of anchors\n",
        "    and GT boxes without exceeding `fg_fraction * num_samples` positives, and\n",
        "    then try to fill the remaining slots with background anchors. We will ignore\n",
        "    \"neutral\" anchors in this sampling as they are not used for training.\n",
        "\n",
        "    Args:\n",
        "        gt_boxes: Tensor of shape `(N, 5)` giving GT box co-ordinates that are\n",
        "            already matched with some anchor boxes (with GT class label at last\n",
        "            dimension). Label -1 means background and -1e8 means meutral.\n",
        "        num_samples: Total anchor-GT pairs with label >= -1 to return.\n",
        "        fg_fraction: The number of subsampled labels with values >= 0 is\n",
        "            `min(num_foreground, int(fg_fraction * num_samples))`. In other\n",
        "            words, if there are not enough fg, the sample is filled with\n",
        "            (duplicate) bg.\n",
        "\n",
        "    Returns:\n",
        "        fg_idx, bg_idx (Tensor):\n",
        "            1D vector of indices. The total length of both is `num_samples` or\n",
        "            fewer. Use these to index anchors, GT boxes, and model predictions.\n",
        "    \"\"\"\n",
        "    foreground = (gt_boxes[:, 4] >= 0).nonzero().squeeze(1)\n",
        "    background = (gt_boxes[:, 4] == -1).nonzero().squeeze(1)\n",
        "\n",
        "    # Protect against not enough foreground examples.\n",
        "    num_fg = min(int(num_samples * fg_fraction), foreground.numel())\n",
        "    num_bg = num_samples - num_fg\n",
        "\n",
        "    # Randomly select positive and negative examples.\n",
        "    perm1 = torch.randperm(foreground.numel(), device=foreground.device)[:num_fg]\n",
        "    perm2 = torch.randperm(background.numel(), device=background.device)[:num_bg]\n",
        "\n",
        "    fg_idx = foreground[perm1]\n",
        "    bg_idx = background[perm2]\n",
        "    return fg_idx, bg_idx"
      ],
      "metadata": {
        "id": "Vs0A8SrztUh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Common Code\n",
        "## This part requires modifications. Detailed instructions are mentioned with some hints\n",
        "\n",
        "\n",
        "def nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Non-maximum suppression removes overlapping bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        boxes: Tensor of shape (N, 4) giving top-left and bottom-right coordinates\n",
        "            of the bounding boxes to perform NMS on.\n",
        "        scores: Tensor of shpe (N, ) giving scores for each of the boxes.\n",
        "        iou_threshold: Discard all overlapping boxes with IoU > iou_threshold\n",
        "\n",
        "    Returns:\n",
        "        keep: torch.long tensor with the indices of the elements that have been\n",
        "            kept by NMS, sorted in decreasing order of scores;\n",
        "            of shape [num_kept_boxes]\n",
        "    \"\"\"\n",
        "\n",
        "    if (not boxes.numel()) or (not scores.numel()):\n",
        "        return torch.zeros(0, dtype=torch.long)\n",
        "\n",
        "    keep = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement non-maximum suppression which iterates the following:     #\n",
        "    #       1. Select the highest-scoring box among the remaining ones,         #\n",
        "    #          which has not been chosen in this step before                    #\n",
        "    #       2. Eliminate boxes with IoU > threshold                             #\n",
        "    #       3. If any boxes remain, GOTO 1                                      #\n",
        "    #       Your implementation should not depend on a specific device type;    #\n",
        "    #       you can use the device of the input if necessary.                   #\n",
        "    # HINT: You can refer to the torchvision library code:                      #\n",
        "    # github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/nms_kernel.cpp\n",
        "    #############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "    ######################################################################\n",
        "    #                            START CODE HERE                        #\n",
        "    ######################################################################\n",
        "\n",
        "    pass\n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "    return keep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lcJNf92woEBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "GWP1vCGL5Eca",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Load several useful packages that are used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "CwVZ26yM5G8U",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def reset_seed(number):\n",
        "    \"\"\"\n",
        "    Reset random seed to the specific number\n",
        "\n",
        "    Inputs:\n",
        "    - number: A seed number to use\n",
        "    \"\"\"\n",
        "    random.seed(number)\n",
        "    torch.manual_seed(number)\n",
        "    return\n",
        "\n",
        "\n",
        "def rel_error(x, y, eps=1e-10):\n",
        "    \"\"\"\n",
        "    Compute the relative error between a pair of tensors x and y,\n",
        "    which is defined as:\n",
        "\n",
        "                            max_i |x_i - y_i]|\n",
        "    rel_error(x, y) = -------------------------------\n",
        "                      max_i |x_i| + max_i |y_i| + eps\n",
        "\n",
        "    Inputs:\n",
        "    - x, y: Tensors of the same shape\n",
        "    - eps: Small positive constant for numeric stability\n",
        "\n",
        "    Returns:\n",
        "    - rel_error: Scalar giving the relative error between x and y\n",
        "    \"\"\"\n",
        "    \"\"\" returns relative error between x and y \"\"\"\n",
        "    top = (x - y).abs().max().item()\n",
        "    bot = (x.abs() + y.abs()).clamp(min=eps).max().item()\n",
        "    return top / bot\n",
        "\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
        "plt.rcParams[\"font.size\"] = 16\n",
        "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
        "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
        "\n",
        "# To download the dataset\n",
        "!pip install wget\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "x7poKGI35JZY",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "Vw3wIuCu5LnU",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Good to go!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "MjJ3uyYBg3Lw",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "W\n",
        "e will use PASCAL VOC 2007 dataset to train our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLQODYNdj3Ou"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "# Set a few constants related to data loading.\n",
        "NUM_CLASSES = 20\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_WORKERS = multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "MmEP5KQJzk0d",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "## Download the dataset in a folder on google drive. Select a folder path to put the data and \n",
        "## replace GOOGLE_DRIVE_PATH with that path\n",
        "\n",
        "train_dataset = VOC2007DetectionTiny(\n",
        "    GOOGLE_DRIVE_PATH, \"train\", image_size=IMAGE_SHAPE[0],\n",
        "    download=False  # True (for the first time)\n",
        ")\n",
        "val_dataset = VOC2007DetectionTiny(GOOGLE_DRIVE_PATH, \"val\", image_size=IMAGE_SHAPE[0])\n",
        "\n",
        "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qMxMcdkj3Ov"
      },
      "source": [
        "Now we wrap these dataset objects with PyTorch dataloaders. The format of output batches will also be same as what you have seen before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x48J5J_Gj3Ov"
      },
      "outputs": [],
      "source": [
        "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
        "# on the main CPU process, suitable for Colab.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Use batch_size = 1 during inference - during inference we do not center crop\n",
        "# the image to detect all objects, hence they may be of different size. It is\n",
        "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "train_loader_iter = iter(train_loader)\n",
        "image_paths, images, gt_boxes = next(train_loader_iter)\n",
        "\n",
        "print(f\"image paths           : {image_paths}\")\n",
        "print(f\"image batch has shape : {images.shape}\")\n",
        "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
        "\n",
        "print(f\"Five boxes per image  :\")\n",
        "print(gt_boxes[:, :5, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "X4WmocEyiXWa",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Visualize PASCAL VOC 2007\n",
        "\n",
        "We will visualize a few images and their GT boxes, just to make sure that everything is loaded properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "ld1s28Z4fyL5",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "# from torchvision import transforms\n",
        "\n",
        "inverse_norm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
        "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
        "    if idx > 2:\n",
        "        break\n",
        "\n",
        "    image = inverse_norm(image)\n",
        "    is_valid = gt_boxes[:, 4] >= 0\n",
        "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQng878xj3Ow"
      },
      "source": [
        "## Backbone with Feature Pyramid Networks (FPN)\n",
        "\n",
        "Faster R-CNN uses a convolutional backbone with FPN in the exact same way as you implemented in FCOS. So you can directly re-use it for this part of the assignment.\n",
        "*italicised text*\n",
        "**NOTE:** Typical state-of-the-art detectors based o nFaster R-CNN use four multi-scale features from different FPN levels — `(p2, p3, p4, p5)` with strides `(4, 8, 16, 32)`.\n",
        "Due to computational limits of Google Colab, we will instead simply use `(p3, p4, p5)` features like FCOS.\n",
        "In all your implementations for this part, you may assume that you will receive features from these three FPN levels (and may hard-code these names as Python strings). Your code will not be tested with `p2` FPN features and you will not lose points."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_MSseg1XkdZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsCw5Dzkj3Ow"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "\n",
        "\n",
        "## This part requires changes\n",
        "\n",
        "class DetectorBackboneWithFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    Detection backbone network: A tiny RegNet model coupled with a Feature\n",
        "    Pyramid Network (FPN). This model takes in batches of input images with\n",
        "    shape `(B, 3, H, W)` and gives features from three different FPN levels\n",
        "    with shapes and total strides upto that level:\n",
        "\n",
        "        - level p3: (out_channels, H /  8, W /  8)      stride =  8\n",
        "        - level p4: (out_channels, H / 16, W / 16)      stride = 16\n",
        "        - level p5: (out_channels, H / 32, W / 32)      stride = 32\n",
        "\n",
        "    NOTE: We could use any convolutional network architecture that progressively\n",
        "    downsamples the input image and couple it with FPN. We use a small enough\n",
        "    backbone that can work with Colab GPU and get decent enough performance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Initialize with ImageNet pre-trained weights.\n",
        "        _cnn = models.regnet_x_400mf(pretrained=True)\n",
        "\n",
        "        # Torchvision models only return features from the last level. Detector\n",
        "        # backbones (with FPN) require intermediate features of different scales.\n",
        "        # So we wrap the ConvNet with torchvision's feature extractor. Here we\n",
        "        # will get output features with names (c3, c4, c5) with same stride as\n",
        "        # (p3, p4, p5) described above.\n",
        "        self.backbone = feature_extraction.create_feature_extractor(\n",
        "            _cnn,\n",
        "            return_nodes={\n",
        "                \"trunk_output.block2\": \"c3\",\n",
        "                \"trunk_output.block3\": \"c4\",\n",
        "                \"trunk_output.block4\": \"c5\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Pass a dummy batch of input images to infer shapes of (c3, c4, c5).\n",
        "        # Features are a dictionary with keys as defined above. Values are\n",
        "        # batches of tensors in NCHW format, that give intermediate features\n",
        "        # from the backbone network.\n",
        "        dummy_out = self.backbone(torch.randn(2, 3, 224, 224))\n",
        "        dummy_out_shapes = [(key, value.shape) for key, value in dummy_out.items()]\n",
        "\n",
        "        print(\"For dummy input images with shape: (2, 3, 224, 224)\")\n",
        "        for level_name, feature_shape in dummy_out_shapes:\n",
        "            print(f\"Shape of {level_name} features: {feature_shape}\")\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Initialize additional Conv layers for FPN.                   #\n",
        "        #                                                                    #\n",
        "        # Create THREE \"lateral\" 1x1 conv layers to transform (c3, c4, c5)   #\n",
        "        # such that they all end up with the same `out_channels`.            #\n",
        "        # Then create THREE \"output\" 3x3 conv layers to transform the merged #\n",
        "        # FPN features to output (p3, p4, p5) features.                      #\n",
        "        # All conv layers must have stride=1 and padding such that features  #\n",
        "        # do not get downsampled due to 3x3 convs.                           #\n",
        "        #                                                                    #\n",
        "        # HINT: You have to use `dummy_out_shapes` defined above to decide   #\n",
        "        # the input/output channels of these layers.                         #\n",
        "        ######################################################################\n",
        "        # This behaves like a Python dict, but makes PyTorch understand that\n",
        "        # there are trainable weights inside it.\n",
        "        # Add THREE lateral 1x1 conv and THREE output 3x3 conv layers.\n",
        "        self.fpn_params = nn.ModuleDict()\n",
        "\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "    @property\n",
        "    def fpn_strides(self):\n",
        "        \"\"\"\n",
        "        Total stride up to the FPN level. For a fixed ConvNet, these values\n",
        "        are invariant to input image size. You may access these values freely\n",
        "        to implement your logic in FCOS / Faster R-CNN.\n",
        "        \"\"\"\n",
        "        return {\"p3\": 8, \"p4\": 16, \"p5\": 32}\n",
        "\n",
        "    def forward(self, images: torch.Tensor):\n",
        "\n",
        "        # Multi-scale features, dictionary with keys: {\"c3\", \"c4\", \"c5\"}.\n",
        "        backbone_feats = self.backbone(images)\n",
        "\n",
        "        fpn_feats = {\"p3\": None, \"p4\": None, \"p5\": None}\n",
        "        ######################################################################\n",
        "        # TODO: Fill output FPN features (p3, p4, p5) using RegNet features  #\n",
        "        # (c3, c4, c5) and FPN conv layers created above.                    #\n",
        "        # HINT: Use `F.interpolate` to upsample FPN features.                #\n",
        "        ######################################################################\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        return fpn_feats\n",
        "\n",
        "\n",
        "\n",
        "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
        "\n",
        "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
        "dummy_images = torch.randn(2, 3, 224, 224)\n",
        "\n",
        "# Collect dummy output.\n",
        "dummy_fpn_feats = backbone(dummy_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "oRc7P-RvRZGZ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Faster R-CNN first stage: Region Proposal Network (RPN)\n",
        "\n",
        "We will now implement the first-stage of Faster R-CNN. It comprises a **Region Proposal Network (RPN)** that learns to predict general _object proposals_, which will then be used by the second stage to make final predictions.\n",
        "\n",
        "**RPN prediction:** An input image is passed through the backbone and we obtain its FPN feature maps `(p3, p4, p5)`.\n",
        "The RPN predicts multiple values at _every location on FPN features_. Faster R-CNN is _anchor-based_ — the model assumes that every location has multiple pre-defined boxes (called \"anchors\") and it predicts two measures per anchor, per FPN location:\n",
        "\n",
        "1. **Objectness:** The likelihood of having _any_ object inside the anchor. This is similar to classification head in FCOS, except that this is _class-agnostic_: it only performs binary foreground/background classification.\n",
        "2. **Box regression deltas:** 4-D \"deltas\" that _transform_ an anchor at that location to a ground-truth box.\n",
        "\n",
        "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
        "\n",
        "**SIDE NOTE:** We will use the more common practice of predicting `k` logits and use a logistic regressor instead of `2k` scores (and 2-way softmax) as shown in Figure. This slightly reduces the number of trainable parameters.\n",
        "\n",
        "This RPN is conceptually quite similar to a one-stage detector like FCOS.\n",
        "The main differences with what you implemented in FCOS are: (1) RPN is anchor-based, and make predictions for multiple anchor boxes instead of location \"points\", (2) it performs class-agnostic object classification, and (3) it excludes centerness regression, which was inntroduced in FCOS itself, years after Faster R-CNN was published.\n",
        "\n",
        "Like we saw in FCOS, each anchor will be matched with a GT box for supervision — we will get to it shortly.\n",
        "For now, let's assume there are some `A` anchor boxes at every FPN location, and implement an RPN module.\n",
        "Structurally, this module is similar to FCOS prediction network.\n",
        "Now follow the instructions in `RPNPredictionNetwork` of `two_stage_detector.py` and implement layers to predict objectness and box regression deltas.\n",
        "Execute the following cell to test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHQ67LTOj3Ox"
      },
      "outputs": [],
      "source": [
        "# from two_stage_detector import RPNPredictionNetwork\n",
        "\n",
        "## This section requires code changes\n",
        "class RPNPredictionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    RPN prediction network that accepts FPN feature maps from different levels\n",
        "    and makes two predictions for every anchor: objectness and box deltas.\n",
        "\n",
        "    Faster R-CNN typically uses (p2, p3, p4, p5) feature maps. We will exclude\n",
        "    p2 for have a small enough model for Colab.\n",
        "\n",
        "    Conceptually this module is quite similar to `FCOSPredictionNetwork`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, in_channels: int, stem_channels: List[int], num_anchors: int = 3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels: Number of channels in input feature maps. This value\n",
        "                is same as the output channels of FPN.\n",
        "            stem_channels: List of integers giving the number of output channels\n",
        "                in each convolution layer of stem layers.\n",
        "            num_anchors: Number of anchor boxes assumed per location (say, `A`).\n",
        "                Faster R-CNN without an FPN uses `A = 9`, anchors with three\n",
        "                different sizes and aspect ratios. With FPN, it is more common\n",
        "                to have a fixed size dependent on the stride of FPN level, hence\n",
        "                `A = 3` is default - with three aspect ratios.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_anchors = num_anchors\n",
        "        ######################################################################\n",
        "        # TODO: Create a stem of alternating 3x3 convolution layers and RELU\n",
        "        # activation modules. RPN shares this stem for objectness and box\n",
        "        # regression (unlike FCOS, that uses separate stems).\n",
        "        #\n",
        "        # Use `in_channels` and `stem_channels` for creating these layers, the\n",
        "        # docstring above tells you what they mean. Initialize weights of each\n",
        "        # conv layer from a normal distribution with mean = 0 and std dev = 0.01\n",
        "        # and all biases with zero. Use conv stride = 1 and zero padding such\n",
        "        # that size of input features remains same: remember we need predictions\n",
        "        # at every location in feature map, we shouldn't \"lose\" any locations.\n",
        "        ######################################################################\n",
        "        # Fill this list. It is okay to use your implementation from\n",
        "        # `FCOSPredictionNetwork` for this code block.\n",
        "        stem_rpn = []\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "\n",
        "        # Wrap the layers defined by student into a `nn.Sequential` module:\n",
        "        self.stem_rpn = nn.Sequential(*stem_rpn)\n",
        "        ######################################################################\n",
        "        # TODO: Create TWO 1x1 conv layers for individually to predict\n",
        "        # objectness and box deltas for every anchor, at every location.\n",
        "        #\n",
        "        # Objectness is obtained by applying sigmoid to its logits. However,\n",
        "        # DO NOT initialize a sigmoid module here. PyTorch loss functions have\n",
        "        # numerically stable implementations with logits.\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace these lines with your code, keep variable names unchanged.\n",
        "        self.pred_obj = None  # Objectness conv\n",
        "        self.pred_box = None  # Box regression conv\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "\n",
        "    def forward(self, feats_per_fpn_level: TensorDict) -> List[TensorDict]:\n",
        "        \"\"\"\n",
        "        Accept FPN feature maps and predict desired quantities for every anchor\n",
        "        at every location. Format the output tensors such that feature height,\n",
        "        width, and number of anchors are collapsed into a single dimension (see\n",
        "        description below in \"Returns\" section) this is convenient for computing\n",
        "        loss and perforning inference.\n",
        "\n",
        "        Args:\n",
        "            feats_per_fpn_level: Features from FPN, keys {\"p3\", \"p4\", \"p5\"}.\n",
        "                Each tensor will have shape `(batch_size, fpn_channels, H, W)`.\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries, each having keys {\"p3\", \"p4\", \"p5\"}:\n",
        "            1. Objectness logits:     `(batch_size, H * W * num_anchors)`\n",
        "            2. Box regression deltas: `(batch_size, H * W * num_anchors, 4)`\n",
        "        \"\"\"\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Iterate over every FPN feature map and obtain predictions using\n",
        "        # the layers defined above. DO NOT apply sigmoid to objectness logits.\n",
        "        ######################################################################\n",
        "        # Fill these with keys: {\"p3\", \"p4\", \"p5\"}, same as input dictionary.\n",
        "        object_logits = {}\n",
        "        boxreg_deltas = {}\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "\n",
        "        return [object_logits, boxreg_deltas]\n",
        "\n",
        "\n",
        "\n",
        "rpn_pred_net = RPNPredictionNetwork(\n",
        "    in_channels=64, stem_channels=[64], num_anchors=3\n",
        ")\n",
        "\n",
        "# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\n",
        "dummy_rpn_obj, dummy_rpn_box = rpn_pred_net(dummy_fpn_feats)\n",
        "\n",
        "# Few expected outputs:\n",
        "# Shape of p4 RPN objectness: torch.Size([2, 196, 3])\n",
        "# Shape of p5 RPN box deltas: torch.Size([2, 49, 12])\n",
        "\n",
        "print(f\"\\nFor dummy input images with shape: {dummy_images.shape}\")\n",
        "for level_name in dummy_fpn_feats.keys():\n",
        "    print(f\"Shape of {level_name} FPN features  : {dummy_fpn_feats[level_name].shape}\")\n",
        "    print(f\"Shape of {level_name} RPN objectness: {dummy_rpn_obj[level_name].shape}\")\n",
        "    print(f\"Shape of {level_name} RPN box deltas: {dummy_rpn_box[level_name].shape}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "etBYc7rbj35F",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "*italicised text*## Anchor-based Training of RPN\n",
        "\n",
        "Now that we implemented this RPN head, our goal is to have it predict _high objectness_ and _accurate box deltas_ for anchors that are likely to contain objects.\n",
        "Similar to first part of our assignment, we need to assign a target GT box to every RPN prediction for training supervision.\n",
        "\n",
        "**Recall FCOS location matching:** FCOS matched every FPN feature map location with a GT box (or marked them background), based on a heuristic that a location whether that location was _inside_ any GT Box.\n",
        "On the other hand, Faster R-CNN is anchor-based: instead of _locations_, it makes predictions with reference to some pre-defined _anchor boxes_, and matches each anchor with a single GT box if they have a high enough Intersection-over-Union (IoU).\n",
        "\n",
        "In the next few cells, we will perform the following steps, which are procedurally very similar to what you have already done with FCOS:\n",
        "\n",
        "1. **Anchor generation:** Generate a set of anchors for every location in FPN feature map.\n",
        "2. **Anchor to GT matching:** Match these anchors with GT boxes based on their IoU-overlap.\n",
        "3. **Format of box deltas:** Implement the tranformation functions to obtain _box deltas_ from GT boxes (model training supervision) and apply deltas to anchors (final proposal boxes for second stage).\n",
        "\n",
        "Let's approach these steps, one at a time.\n",
        "\n",
        "### Anchor Generation\n",
        "\n",
        "Recall that you already implemented a function to get the absolute image co-ordinates of FPN feature map locations, for FCOS — in `get_fpn_location_coords` of `common.py`.\n",
        "First we need to form multiple anchor boxes centered at these locations.\n",
        "RPN defines square anchor boxes of size `scale * stride` at every location, where `stride` is the FPN level stride, and `scale` is a hyperparameter.\n",
        "For example, anchor boxes for P5 level (`stride = 32`), with `scale = 2` will be boxes of `(64 x 64)` pixels.\n",
        "RPN also considers anchors of different aspect ratios, apart from square anchor boxes —\n",
        "follow the instructions in `generate_fpn_anchors` of `two_stage_detector.py` to implement creation of multiple anchor boxes per location.\n",
        "\n",
        "Execute the next cell to verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "O5w-EUJekJj-",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "# from common import get_fpn_location_coords\n",
        "# from two_stage_detector import generate_fpn_anchors\n",
        "\n",
        "## This section requires code changes\n",
        "\n",
        "def get_fpn_location_coords(\n",
        "    shape_per_fpn_level: Dict[str, Tuple],\n",
        "    strides_per_fpn_level: Dict[str, int],\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    device: str = \"cpu\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Map every location in FPN feature map to a point on the image. This point\n",
        "    represents the center of the receptive field of this location. We need to\n",
        "    do this for having a uniform co-ordinate representation of all the locations\n",
        "    across FPN levels, and GT boxes.\n",
        "\n",
        "    Args:\n",
        "        shape_per_fpn_level: Shape of the FPN feature level, dictionary of keys\n",
        "            {\"p3\", \"p4\", \"p5\"} and feature shapes `(B, C, H, W)` as values.\n",
        "        strides_per_fpn_level: Dictionary of same keys as above, each with an\n",
        "            integer value giving the stride of corresponding FPN level.\n",
        "            See `backbone.py` for more details.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, torch.Tensor]\n",
        "            Dictionary with same keys as `shape_per_fpn_level` and values as\n",
        "            tensors of shape `(H * W, 2)` giving `(xc, yc)` co-ordinates of the\n",
        "            centers of receptive fields of the FPN locations, on input image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set these to `(N, 2)` Tensors giving absolute location co-ordinates.\n",
        "    location_coords = {\n",
        "        level_name: None for level_name, _ in shape_per_fpn_level.items()\n",
        "    }\n",
        "\n",
        "    for level_name, feat_shape in shape_per_fpn_level.items():\n",
        "        level_stride = strides_per_fpn_level[level_name]\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "        pass\n",
        "        ######################################################################\n",
        "        #                            END OF YOUR CODE                        #\n",
        "        ######################################################################\n",
        "        ######################################################################\n",
        "        # TODO: Implement logic to get location co-ordinates below.          #\n",
        "\n",
        "    return location_coords\n",
        "\n",
        "\n",
        "\n",
        "def generate_fpn_anchors(\n",
        "    locations_per_fpn_level: TensorDict,\n",
        "    strides_per_fpn_level: Dict[str, int],\n",
        "    stride_scale: int,\n",
        "    aspect_ratios: List[float] = [0.5, 1.0, 2.0],\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate multiple anchor boxes at every location of FPN level. Anchor boxes\n",
        "    should be in XYXY format and they should be centered at the given locations.\n",
        "\n",
        "    Args:\n",
        "        locations_per_fpn_level: Centers at different levels of FPN (p3, p4, p5),\n",
        "            that are already projected to absolute co-ordinates in input image\n",
        "            dimension. Dictionary of three keys: (p3, p4, p5) giving tensors of\n",
        "            shape `(H * W, 2)` where H, W is the size of FPN feature map.\n",
        "        strides_per_fpn_level: Dictionary of same keys as above, each with an\n",
        "            integer value giving the stride of corresponding FPN level.\n",
        "            See `common.py` for more details.\n",
        "        stride_scale: Size of square anchor at every FPN levels will be\n",
        "            `(this value) * (FPN level stride)`. Default is 4, which will make\n",
        "            anchor boxes of size (32x32), (64x64), (128x128) for FPN levels\n",
        "            p3, p4, and p5 respectively.\n",
        "        aspect_ratios: Anchor aspect ratios to consider at every location. We\n",
        "            consider anchor area to be `(stride_scale * FPN level stride) ** 2`\n",
        "            and set new width and height of anchors at every location:\n",
        "                new_width = sqrt(area / aspect ratio)\n",
        "                new_height = area / new_width\n",
        "\n",
        "    Returns:\n",
        "        TensorDict\n",
        "            Dictionary with same keys as `locations_per_fpn_level` and values as\n",
        "            tensors of shape `(HWA, 4)` giving anchors for all locations\n",
        "            per FPN level, each location having `A = len(aspect_ratios)` anchors.\n",
        "            All anchors are in XYXY format and their centers align with locations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set these to `(N, A, 4)` Tensors giving anchor boxes in XYXY format.\n",
        "    anchors_per_fpn_level = {\n",
        "        level_name: None for level_name, _ in locations_per_fpn_level.items()\n",
        "    }\n",
        "\n",
        "    for level_name, locations in locations_per_fpn_level.items():\n",
        "        level_stride = strides_per_fpn_level[level_name]\n",
        "\n",
        "        # List of `A = len(aspect_ratios)` anchor boxes.\n",
        "        anchor_boxes = []\n",
        "        for aspect_ratio in aspect_ratios:\n",
        "            ##################################################################\n",
        "            # TODO: Implement logic for anchor boxes below. Write vectorized\n",
        "            # implementation to generate anchors for a single aspect ratio.\n",
        "            # Fill `anchor_boxes` list above.\n",
        "            #\n",
        "            # Calculate resulting width and height of the anchor box as per\n",
        "            # `stride_scale` and `aspect_ratios` definitions. Then shift the\n",
        "            # locations to get top-left and bottom-right co-ordinates.\n",
        "            ##################################################################\n",
        "            # Replace \"pass\" statement with your code\n",
        "\n",
        "            ######################################################################\n",
        "            #                            START YOUR CODE                        #\n",
        "            ######################################################################\n",
        "\n",
        "            pass\n",
        "            ######################################################################\n",
        "            #                            END OF YOUR CODE                        #\n",
        "            ######################################################################\n",
        "\n",
        "\n",
        "        # shape: (A, H * W, 4)\n",
        "        anchor_boxes = torch.stack(anchor_boxes)\n",
        "        # Bring `H * W` first and collapse those dimensions.\n",
        "        anchor_boxes = anchor_boxes.permute(1, 0, 2).contiguous().view(-1, 4)\n",
        "        anchors_per_fpn_level[level_name] = anchor_boxes\n",
        "\n",
        "    return anchors_per_fpn_level\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sanity check: Get 2x2 location co-ordinates of p5 (original shape is 7x7).\n",
        "locations = get_fpn_location_coords(\n",
        "    shape_per_fpn_level={\"p5\": (2, 64, 2, 2)}, strides_per_fpn_level={\"p5\": 32}\n",
        ")\n",
        "\n",
        "print(\"P5 locations:\\n\", locations[\"p5\"])\n",
        "\n",
        "# Generate anchors for these locations.\n",
        "anchors = generate_fpn_anchors(\n",
        "    locations_per_fpn_level=locations,\n",
        "    strides_per_fpn_level={\"p5\": 32},\n",
        "    stride_scale=2,\n",
        "    aspect_ratios=[0.5, 1.0, 2.0],\n",
        ")\n",
        "\n",
        "print(\"P5 anchors with different aspect ratios:\")\n",
        "print(\"P5 1:2 anchors:\\n\", anchors[\"p5\"][0::3, :], \"\\n\")\n",
        "# Expected (any ordering is fine):\n",
        "# [-29.2548,  -6.6274,  61.2548,  38.6274]\n",
        "# [-29.2548,  25.3726,  61.2548,  70.6274]\n",
        "# [  2.7452,  -6.6274,  93.2548,  38.6274]\n",
        "# [  2.7452,  25.3726,  93.2548,  70.6274]\n",
        "\n",
        "print(\"P5 1:1 anchors:\\n\", anchors[\"p5\"][1::3, :], \"\\n\")\n",
        "# Expected (any ordering is fine):\n",
        "# [-16., -16.,  48.,  48.]\n",
        "# [-16.,  16.,  48.,  80.]\n",
        "# [ 16., -16.,  80.,  48.]\n",
        "# [ 16.,  16.,  80.,  80.]\n",
        "\n",
        "print(\"P5 2:1 anchors:\\n\", anchors[\"p5\"][2::3, :], \"\\n\")\n",
        "# Similar to 1:2 anchors, but with flipped co-ordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUnBiT5Yj3Oy"
      },
      "source": [
        "### Matching anchor boxes with GT boxes\n",
        "\n",
        "Similar to FCOS, we will now match these generated anchors with GT boxes. Faster R-CNN matches some `N` anchor boxes with `M` GT boxes by applying a simple rule:\n",
        "\n",
        "> Anchor box $N_i$ is matched with box $M_i$ if it has an IoU overlap higher than 0.6 with that box. For multiple such GT boxes, the anchor is assigned with the GT box that has the highest IoU. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
        "\n",
        "**NOTE:** _Faster R-CNN uses 0.7 default threshold_ as mentioned in the lecture slides. We use a lower threeshold to increase the number of positive matches for sampling — this helps in speeding up training in a resource constrained setting like Google Colab.\n",
        "\n",
        "Anchor boxes with `IoU < 0.3` with ALL GT boxes is assigned background GT box `(-1, -1, -1, -1, -1)`. All other anchors with IoU between `(0.3, 0.6)` are considered \"neutral\" and ignored during training. This matching differs from FCOS, which assigns ALL anchors to either object or background — the \"neutral\" Faster R-CNN anchors cause wasted computation, and removing this redundancy would overly complicate our implementation.\n",
        "\n",
        "We have implemented this matching procedure for you — see `rcnn_match_anchors_to_gt` of `two_stage_detector.py`.\n",
        "Read its documentation to understand its input/output format, it is slightly different than `fcos_match_locations_to_gt`.\n",
        "It serves the same purpose as location matching in FCOS — to define GT targets for model predictions during training.\n",
        "\n",
        "This function internally requires IoU computation between all anchors and GT boxes — which you have to implement.\n",
        "Follow the instructions in `two_stage_detector.py` to implement IoU computation, and execute the next cell for a sanity check — you should observe an error of `1e-7` or less"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "fK_USCuaXSzh",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "## This section requires code changes\n",
        "\n",
        "def iou(boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute intersection-over-union (IoU) between pairs of box tensors. Input\n",
        "    box tensors must in XYXY format.\n",
        "\n",
        "    Args:\n",
        "        boxes1: Tensor of shape `(M, 4)` giving a set of box co-ordinates.\n",
        "        boxes2: Tensor of shape `(N, 4)` giving another set of box co-ordinates.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor\n",
        "            Tensor of shape (M, N) with `iou[i, j]` giving IoU between i-th box\n",
        "            in `boxes1` and j-th box in `boxes2`.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ##########################################################################\n",
        "    # TODO: Implement the IoU function here.                                 #\n",
        "    ##########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "    ######################################################################\n",
        "    #                            START YOUR CODE                        #\n",
        "    ######################################################################\n",
        "\n",
        "    pass\n",
        "    ##########################################################################\n",
        "    #                             END OF YOUR CODE                           #\n",
        "    ##########################################################################\n",
        "    return iou\n",
        "\n",
        "\n",
        "boxes1 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n",
        "boxes2 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n",
        "\n",
        "expected_iou = torch.Tensor(\n",
        "    [[1.0, 0.0625, 0.25], [0.0625, 0.0, 0.052631579], [0.0625, 1.0, 0.052631579]]\n",
        ")\n",
        "result_iou = iou(boxes1, boxes2)\n",
        "\n",
        "print(\"Relative error:\", rel_error(expected_iou, result_iou))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id2XlvGWj3Oy"
      },
      "source": [
        "### Visualizing matched GT boxes\n",
        "\n",
        "Now we apply our anchor matching function and visualize one GT box with a random matched positive anchor box.\n",
        "You may try different images by indexing `train_dataset` below,\n",
        "make sure to try different FPN levels as certain images may not get any matched GT boxes due to their size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "wPvX4TrgaLD8",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# from common import get_fpn_location_coords\n",
        "# from two_stage_detector import generate_fpn_anchors, rcnn_match_anchors_to_gt\n",
        "\n",
        "\n",
        "# Sanity check: Match anchors of p4 level with GT boxes of first image\n",
        "# in the training dataset.\n",
        "_, image, gt_boxes = train_dataset[0]\n",
        "\n",
        "FPN_LEVEL = \"p4\"\n",
        "FPN_STRIDE = 16\n",
        "locations = get_fpn_location_coords(\n",
        "    shape_per_fpn_level={FPN_LEVEL: (2, 64, 224 // FPN_STRIDE, 224 // FPN_STRIDE)},\n",
        "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE}\n",
        ")\n",
        "# Generate anchors for these locations.\n",
        "anchors = generate_fpn_anchors(\n",
        "    locations_per_fpn_level=locations,\n",
        "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE},\n",
        "    stride_scale=8,  # Default value used by Faster R-CNN\n",
        "    aspect_ratios=[0.5, 1.0, 2.0],\n",
        ")\n",
        "\n",
        "matched_gt_boxes = rcnn_match_anchors_to_gt(\n",
        "    anchors[FPN_LEVEL], gt_boxes, iou_thresholds=(0.3, 0.6)\n",
        ")\n",
        "\n",
        "# Flatten anchors and matched boxes:\n",
        "anchors_p4 = anchors[FPN_LEVEL].view(-1, 4)\n",
        "matched_boxes_p4 = matched_gt_boxes.view(-1, 5)\n",
        "\n",
        "# Visualize one selected anchor and its matched GT box.\n",
        "# NOTE: Run this cell multiple times to see different matched anchors. For car\n",
        "# image, p3/5 will not work because the GT box was already assigned to p4.\n",
        "fg_idxs_p4 = (matched_boxes_p4[:, 4] > 0).nonzero()\n",
        "fg_idx = random.choice(fg_idxs_p4)\n",
        "\n",
        "# Combine both boxes for visualization:\n",
        "dummy_vis_boxes = [anchors_p4[fg_idx][0], matched_boxes_p4[fg_idx][0]]\n",
        "\n",
        "print(\"Unlabeled red box is positive anchor:\")\n",
        "detection_visualizer(\n",
        "    inverse_norm(image),\n",
        "    val_dataset.idx_to_class,\n",
        "    bbox=dummy_vis_boxes,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "XW_Zek3_dgfF",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### GT Targets for box regression\n",
        "\n",
        "Now we work on the third and final component needed to train our RPN — we define transformation functions for box regression deltas :\n",
        "\n",
        "> 1. `fcos_get_deltas_from_locations`: Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
        "> 2. `fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
        "\n",
        "Here you will implement similar transformation functions for R-CNN. You can use the ideas from the slides frmo the class and implement two functions:\n",
        "\n",
        "1. `rcnn_get_deltas_from_anchors`: Accepts anchor boxes and GT boxes, and returns deltas. Required for training supervision.\n",
        "2. `rcnn_apply_deltas_to_anchors`: Accepts predicted deltas and anchor boxes, and returns predicted boxes. Required during inference.\n",
        "\n",
        "Run the following cell to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "MX2JCaOf0768",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "## This section code changes \n",
        "\n",
        "\n",
        "def rcnn_get_deltas_from_anchors(\n",
        "    anchors: torch.Tensor, gt_boxes: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get box regression deltas that transform `anchors` to `gt_boxes`. These\n",
        "    deltas will become GT targets for box regression. Unlike FCOS, the deltas\n",
        "    are in `(dx, dy, dw, dh)` format that represent offsets to anchor centers\n",
        "    and scaling factors for anchor size. Box regression is only supervised by\n",
        "    foreground anchors. If GT boxes are \"background/neutral\", then deltas\n",
        "    must be `(-1e8, -1e8, -1e8, -1e8)` (just some LARGE negative number).\n",
        "\n",
        "    Args:\n",
        "        anchors: Tensor of shape `(N, 4)` giving anchors boxes in XYXY format.\n",
        "        gt_boxes: Tensor of shape `(N, 4)` giving matching GT boxes.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor\n",
        "            Tensor of shape `(N, 4)` giving anchor deltas.\n",
        "    \"\"\"\n",
        "    ##########################################################################\n",
        "    # TODO: Implement the logic to get deltas.                               #\n",
        "    # Remember to set the deltas of \"background/neutral\" GT boxes to -1e8    #\n",
        "    ##########################################################################\n",
        "    deltas = None\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "    ######################################################################\n",
        "    #                            START YOUR CODE                        #\n",
        "    ######################################################################\n",
        "\n",
        "    pass\n",
        "    ##########################################################################\n",
        "    #                             END OF YOUR CODE                           #\n",
        "    ##########################################################################\n",
        "    \n",
        "    return deltas\n",
        "\n",
        "\n",
        "\n",
        "def rcnn_apply_deltas_to_anchors(\n",
        "    deltas: torch.Tensor, anchors: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Implement the inverse of `rcnn_get_deltas_from_anchors` here.\n",
        "\n",
        "    Args:\n",
        "        deltas: Tensor of shape `(N, 4)` giving box regression deltas.\n",
        "        anchors: Tensor of shape `(N, 4)` giving anchors to apply deltas on.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor\n",
        "            Same shape as deltas and locations, giving the resulting boxes in\n",
        "            XYXY format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Clamp dw and dh such that they would transform a 8px box no larger than\n",
        "    # 224px. This is necessary for numerical stability as we apply exponential.\n",
        "    scale_clamp = math.log(224 / 8)\n",
        "    deltas[:, 2] = torch.clamp(deltas[:, 2], max=scale_clamp)\n",
        "    deltas[:, 3] = torch.clamp(deltas[:, 3], max=scale_clamp)\n",
        "\n",
        "    ##########################################################################\n",
        "    # TODO: Implement the transformation logic to get output boxes.          #\n",
        "    ##########################################################################\n",
        "    output_boxes = None\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "    ######################################################################\n",
        "    #                            START YOUR CODE                        #\n",
        "    ######################################################################\n",
        "\n",
        "    pass\n",
        "    ##########################################################################\n",
        "    #                             END OF YOUR CODE                           #\n",
        "    ##########################################################################\n",
        "\n",
        "    return output_boxes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Three hard-coded anchor boxes and GT boxes that have a fairly high overlap.\n",
        "# Add a dummy class ID = 1 indicating foreground\n",
        "input_anchors = torch.Tensor(\n",
        "    [[20, 40, 80, 90], [10, 10, 50, 50], [120, 100, 200, 200]]\n",
        ")\n",
        "input_boxes = torch.Tensor(\n",
        "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
        ")\n",
        "\n",
        "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
        "# and applying them back to anchors should give us the same boxes.\n",
        "_deltas = rcnn_get_deltas_from_anchors(input_anchors, input_boxes)\n",
        "output_boxes = rcnn_apply_deltas_to_anchors(_deltas, input_anchors)\n",
        "\n",
        "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
        "\n",
        "# Another check: deltas for GT class label = -1 should be -1e8\n",
        "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
        "input_anchor = torch.Tensor([[100, 100, 200, 200]])\n",
        "\n",
        "_deltas = rcnn_get_deltas_from_anchors(input_anchor, background_box)\n",
        "output_box = rcnn_apply_deltas_to_anchors(_deltas, input_anchor)\n",
        "\n",
        "print(\"Background deltas should be all -1e8  :\", _deltas)\n",
        "print(\"Output box should be -1e8 or lower    :\", output_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "dlO2IUCnt4zu",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "With all predictions assigned with GT targets, we will proceed to compute losses for training the RPN.\n",
        "Recall that you used [Focal Loss](https://arxiv.org/abs/1708.02002) for classification and L1 loss for box regression in FCOS.\n",
        "Here, you will use L1 loss for box regression, similar to FCOS.\n",
        "\n",
        "**Objectness classification loss:** Focal Loss was proposed in RetinaNet (2017) to deal with heavy class imbalance caused by \"background\". Faster R-CNN predates this paper — it dealt with class imbalance by randomly sampling roughly equal amount of foreground-background anchors to train RPN. We have implemented a very simple sampling function for you in `sample_rpn_training` function of `two_stage_detector.py` — you may directly use it while you piece all these components (coming up next).\n",
        "\n",
        "**Total loss** is the sum of both loss components _per sampled anchor_, averaged by total number of foreground + background anchors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "2eSleGX9yTeo",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sanity check: dummy predictions from model - box regression deltas and\n",
        "# objectness logits for two anchors.\n",
        "# shape: (batch_size, HWA, 4 or 1)\n",
        "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
        "dummy_pred_obj_logits = torch.randn(1, 2, 1)\n",
        "\n",
        "# Dummy deltas and objectness targets. Let the second box be background.\n",
        "# Dummy GT boxes (matched with both anchors).\n",
        "dummy_gt_deltas = torch.randn_like(dummy_pred_boxreg_deltas)\n",
        "dummy_gt_deltas[:, 1, :] = -1e8\n",
        "\n",
        "# Background objectness targets should be 0 (not -1), and foreground\n",
        "# should be 1. Neutral anchors will not occur here due to sampling.\n",
        "dummy_gt_objectness = torch.Tensor([1, 0])\n",
        "\n",
        "# Note that loss is not multiplied with 0.25 here:\n",
        "loss_box = F.l1_loss(\n",
        "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# No loss for background anchors:\n",
        "loss_box[dummy_gt_deltas == -1e8] *= 0.0\n",
        "print(\"Box regression loss (L1):\", loss_box)\n",
        "\n",
        "# Now calculate objectness loss.\n",
        "loss_obj = F.binary_cross_entropy_with_logits(\n",
        "    dummy_pred_obj_logits.view(-1), dummy_gt_objectness, reduction=\"none\"\n",
        ")\n",
        "print(\"Objectness loss (BCE):\", loss_obj)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4071PE_6xLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "AiPfXUHPupDE",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Putting it all together: RPN module\n",
        "\n",
        "Now you will put together all the things you have implemented into the `RPN` class in `two_stage_detector.py`.\n",
        "Implement `forward` and `predict_proposals` functions of this module — you have already done most of the heavy lifting, you simply need to call all the functions in a correct way!\n",
        "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
        "\n",
        "**TIP:** It may help if you draw analogies between the implementation logic in this module vs FCOS (`RPN.predict_proposals()` -> `FCOS.inference()`).\n",
        "\n",
        "## Overfit small data\n",
        "\n",
        "In Faster R-CNN, the RPN is trained jointly with the second-stage network.\n",
        "However, to test our RPN implementation, we will first train just the RPN — this is basically a class-agnostic FCOS without centerness.\n",
        "We will use the `train_detector` function that we used for training FCOS.\n",
        "You can read its implementation in `a4_helper.py`. \n",
        "\n",
        "The loss should generally do down, however the forward pass here is a bit slower than FCOS."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## This section requires code changes\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    \"\"\"\n",
        "    Region Proposal Network: First stage of Faster R-CNN detector.\n",
        "\n",
        "    This class puts together everything you implemented so far. It accepts FPN\n",
        "    features as input and uses `RPNPredictionNetwork` to predict objectness and\n",
        "    box reg deltas. Computes proposal boxes for second stage (during both\n",
        "    training and inference) and losses during training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fpn_channels: int,\n",
        "        stem_channels: List[int],\n",
        "        batch_size_per_image: int,\n",
        "        anchor_stride_scale: int = 8,\n",
        "        anchor_aspect_ratios: List[int] = [0.5, 1.0, 2.0],\n",
        "        anchor_iou_thresholds: Tuple[int, int] = (0.3, 0.6),\n",
        "        nms_thresh: float = 0.7,\n",
        "        pre_nms_topk: int = 400,\n",
        "        post_nms_topk: int = 100,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch_size_per_image: Anchors per image to sample for training.\n",
        "            nms_thresh: IoU threshold for NMS - unlike FCOS, this is used\n",
        "                during both, training and inference.\n",
        "            pre_nms_topk: Number of top-K proposals to select before applying\n",
        "                NMS, per FPN level. This helps in speeding up NMS.\n",
        "            post_nms_topk: Number of top-K proposals to select after applying\n",
        "                NMS, per FPN level. NMS is obviously going to be class-agnostic.\n",
        "\n",
        "        Refer explanations of remaining args in the classes/functions above.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.pred_net = RPNPredictionNetwork(\n",
        "            fpn_channels, stem_channels, num_anchors=len(anchor_aspect_ratios)\n",
        "        )\n",
        "        # Record all input arguments:\n",
        "        self.batch_size_per_image = batch_size_per_image\n",
        "        self.anchor_stride_scale = anchor_stride_scale\n",
        "        self.anchor_aspect_ratios = anchor_aspect_ratios\n",
        "        self.anchor_iou_thresholds = anchor_iou_thresholds\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.pre_nms_topk = pre_nms_topk\n",
        "        self.post_nms_topk = post_nms_topk\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        feats_per_fpn_level: TensorDict,\n",
        "        strides_per_fpn_level: TensorDict,\n",
        "        gt_boxes: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        # Get batch size from FPN feats:\n",
        "        num_images = feats_per_fpn_level[\"p3\"].shape[0]\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Implement the training forward pass. Follow these steps:\n",
        "        #   1. Pass the FPN features per level to the RPN prediction network.\n",
        "        #   2. Generate anchor boxes for all FPN levels.\n",
        "        #\n",
        "        # HINT: You have already implemented everything, just have to call the\n",
        "        # appropriate functions.\n",
        "        ######################################################################\n",
        "        # Feel free to delete this line: (but keep variable names same)\n",
        "        pred_obj_logits, pred_boxreg_deltas, anchors_per_fpn_level = (\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        pass\n",
        "        ##########################################################################\n",
        "        #                             END OF YOUR CODE                           #\n",
        "        ##########################################################################\n",
        "       \n",
        "        # We will fill three values in this output dict - \"proposals\",\n",
        "        # \"loss_rpn_box\" (training only), \"loss_rpn_obj\" (training only)\n",
        "        output_dict = {}\n",
        "\n",
        "        # Get image height and width according to feature sizes and strides.\n",
        "        # We need these to clamp proposals (These should be (224, 224) but we\n",
        "        # avoid hard-coding them).\n",
        "        img_h = feats_per_fpn_level[\"p3\"].shape[2] * strides_per_fpn_level[\"p3\"]\n",
        "        img_w = feats_per_fpn_level[\"p3\"].shape[3] * strides_per_fpn_level[\"p3\"]\n",
        "\n",
        "        # STUDENT: Implement this method before moving forward with the rest\n",
        "        # of this `forward` method.\n",
        "        output_dict[\"proposals\"] = self.predict_proposals(\n",
        "            anchors_per_fpn_level,\n",
        "            pred_obj_logits,\n",
        "            pred_boxreg_deltas,\n",
        "            (img_w, img_h),\n",
        "        )\n",
        "        # Return here during inference - loss computation not required.\n",
        "        if not self.training:\n",
        "            return output_dict\n",
        "\n",
        "        # ... otherwise continue loss computation:\n",
        "        ######################################################################\n",
        "        # Match the generated anchors with provided GT boxes. This\n",
        "        # function is not batched so you may use a for-loop, like FCOS.\n",
        "        ######################################################################\n",
        "        # Combine anchor boxes from all FPN levels - we do not need any\n",
        "        # distinction of boxes across different levels (for training).\n",
        "        anchor_boxes = self._cat_across_fpn_levels(anchors_per_fpn_level, dim=0)\n",
        "\n",
        "        # Get matched GT boxes (list of B tensors, each of shape `(H*W*A, 5)`\n",
        "        # giving matching GT boxes to anchor boxes). Fill this list:\n",
        "        matched_gt_boxes = []\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        \n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        pass\n",
        "        ##########################################################################\n",
        "        #                             END OF YOUR CODE                           #\n",
        "        ##########################################################################\n",
        "        \n",
        "        # Combine matched boxes from all images to a `(B, HWA, 5)` tensor.\n",
        "        matched_gt_boxes = torch.stack(matched_gt_boxes, dim=0)\n",
        "\n",
        "        # Combine predictions across all FPN levels.\n",
        "        pred_obj_logits = self._cat_across_fpn_levels(pred_obj_logits)\n",
        "        pred_boxreg_deltas = self._cat_across_fpn_levels(pred_boxreg_deltas)\n",
        "\n",
        "        if self.training:\n",
        "            # Repeat anchor boxes `batch_size` times so there is a 1:1\n",
        "            # correspondence with GT boxes.\n",
        "            anchor_boxes = anchor_boxes.unsqueeze(0).repeat(num_images, 1, 1)\n",
        "            anchor_boxes = anchor_boxes.contiguous().view(-1, 4)\n",
        "\n",
        "            # Collapse `batch_size`, and `HWA` to a single dimension so we have\n",
        "            # simple `(-1, 4 or 5)` tensors. This simplifies loss computation.\n",
        "            matched_gt_boxes = matched_gt_boxes.view(-1, 5)\n",
        "            pred_obj_logits = pred_obj_logits.view(-1)\n",
        "            pred_boxreg_deltas = pred_boxreg_deltas.view(-1, 4)\n",
        "\n",
        "            ##################################################################\n",
        "            # TODO: Compute training losses. Follow three steps in order:\n",
        "            #   1. Sample a few anchor boxes for training. Pass the variable\n",
        "            #      `matched_gt_boxes` to `sample_rpn_training` function and\n",
        "            #      use those indices to get subset of predictions and targets.\n",
        "            #      RPN samples 50-50% foreground/background anchors, unless\n",
        "            #      there aren't enough foreground anchors.\n",
        "            #\n",
        "            #   2. Compute GT targets for box regression (you have implemented\n",
        "            #      the transformation function already).\n",
        "            #\n",
        "            #   3. Calculate objectness and box reg losses per sampled anchor.\n",
        "            #      Remember to set box loss for \"background\" anchors to 0.\n",
        "            ##################################################################\n",
        "            # Feel free to delete this line: (but keep variable names same)\n",
        "            loss_obj, loss_box = None, None\n",
        "            # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "            ######################################################################\n",
        "            #                            START YOUR CODE                        #\n",
        "            ######################################################################\n",
        "\n",
        "            pass\n",
        "            ##########################################################################\n",
        "            #                             END OF YOUR CODE                           #\n",
        "            ##########################################################################\n",
        "            \n",
        "\n",
        "            # Sum losses and average by num(foreground + background) anchors.\n",
        "            # In training code, we simply add these two and call `.backward()`\n",
        "            total_batch_size = self.batch_size_per_image * num_images\n",
        "            output_dict[\"loss_rpn_obj\"] = loss_obj.sum() / total_batch_size\n",
        "            output_dict[\"loss_rpn_box\"] = loss_box.sum() / total_batch_size\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    @torch.no_grad()  # Don't track gradients in this function.\n",
        "    def predict_proposals(\n",
        "        self,\n",
        "        anchors_per_fpn_level: Dict[str, torch.Tensor],\n",
        "        pred_obj_logits: Dict[str, torch.Tensor],\n",
        "        pred_boxreg_deltas: Dict[str, torch.Tensor],\n",
        "        image_size: Tuple[int, int],  # (width, height)\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Predict proposals for a batch of images for the second stage. Other\n",
        "        input arguments are same as those computed in `forward` method. This\n",
        "        method should not be called from anywhere except from inside `forward`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor\n",
        "                proposals: Tensor of shape `(keep_topk, 4)` giving *absolute*\n",
        "                    XYXY co-ordinates of predicted proposals. These will serve\n",
        "                    as anchor boxes for the second stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Gather proposals from all FPN levels in this list.\n",
        "        proposals_all_levels = {\n",
        "            level_name: None for level_name, _ in anchors_per_fpn_level.items()\n",
        "        }\n",
        "        for level_name in anchors_per_fpn_level.keys():\n",
        "\n",
        "            # Get anchor boxes and predictions from a single level.\n",
        "            level_anchors = anchors_per_fpn_level[level_name]\n",
        "\n",
        "            # shape: (batch_size, HWA), (batch_size, HWA, 4)\n",
        "            level_obj_logits = pred_obj_logits[level_name]\n",
        "            level_boxreg_deltas = pred_boxreg_deltas[level_name]\n",
        "\n",
        "            # Fill proposals per image, for this FPN level, in this list.\n",
        "            level_proposals_per_image = []\n",
        "            for _batch_idx in range(level_obj_logits.shape[0]):\n",
        "                ##############################################################\n",
        "                # TODO: Perform the following steps in order:\n",
        "                #   1. Transform the anchors to proposal boxes using predicted\n",
        "                #      box deltas, clamp to image height and width.\n",
        "                #   2. Sort all proposals by their predicted objectness, and\n",
        "                #      retain `self.pre_nms_topk` proposals. This speeds up\n",
        "                #      our NMS computation. HINT: `torch.topk`\n",
        "                #   3. Apply NMS and retain `keep_topk_per_level` proposals\n",
        "                #      per image, per level.\n",
        "                #\n",
        "                # NOTE: Your `nms` method may be slow for training - you may\n",
        "                # use `torchvision.ops.nms` with exact same input arguments,\n",
        "                # to speed up training. We will grade your `nms` implementation\n",
        "                # separately; you will NOT lose points if you don't use it here.\n",
        "                #\n",
        "                # Note that deltas, anchor boxes, and objectness logits have\n",
        "                # different shapes, you need to make some intermediate views.\n",
        "                ##############################################################\n",
        "                # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "                ######################################################################\n",
        "                #                            START YOUR CODE                        #\n",
        "                ######################################################################\n",
        "\n",
        "                pass\n",
        "                ##########################################################################\n",
        "                #                             END OF YOUR CODE                           #\n",
        "                ##########################################################################\n",
        "                \n",
        "            # Collate proposals from individual images. Do not stack these\n",
        "            # tensors, they may have different shapes since few images or\n",
        "            # levels may have less than `post_nms_topk` proposals. We could\n",
        "            # pad these tensors but there's no point - they will be used by\n",
        "            # `torchvision.ops.roi_align` in second stage which operates\n",
        "            # with lists, not batched tensors.\n",
        "            proposals_all_levels[level_name] = level_proposals_per_image\n",
        "\n",
        "        return proposals_all_levels\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_across_fpn_levels(\n",
        "        dict_with_fpn_levels: Dict[str, torch.Tensor], dim: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a dict of tensors across FPN levels {\"p3\", \"p4\", \"p5\"} to a\n",
        "        single tensor. Values could be anything - batches of image features,\n",
        "        GT targets, etc.\n",
        "        \"\"\"\n",
        "        return torch.cat(list(dict_with_fpn_levels.values()), dim=dim)\n"
      ],
      "metadata": {
        "id": "m06L3HJK6yR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "YTObddiog9wJ",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "reset_seed(0)\n",
        "\n",
        "\n",
        "# Take equally spaced examples from training dataset to make a subset.\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    train_dataset,\n",
        "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
        ")\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "\n",
        "def infinite_loader(loader):\n",
        "    \"\"\"Get an infinite stream of batches from a data loader.\"\"\"\n",
        "    while True:\n",
        "        yield from loader\n",
        "\n",
        "        \n",
        "def train_detector(\n",
        "    detector,\n",
        "    train_loader,\n",
        "    learning_rate: float = 5e-3,\n",
        "    weight_decay: float = 1e-4,\n",
        "    max_iters: int = 5000,\n",
        "    log_period: int = 20,\n",
        "    device: str = \"cpu\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the detector. We use SGD with momentum and step decay.\n",
        "    \"\"\"\n",
        "\n",
        "    detector.to(device=device)\n",
        "\n",
        "    # Optimizer: use SGD with momentum.\n",
        "    # Use SGD with momentum:\n",
        "    optimizer = optim.SGD(\n",
        "        filter(lambda p: p.requires_grad, detector.parameters()),\n",
        "        momentum=0.9,\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "    # LR scheduler: use step decay at 70% and 90% of training iters.\n",
        "    lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=[int(0.6 * max_iters), int(0.9 * max_iters)]\n",
        "    )\n",
        "\n",
        "    # Keep track of training loss for plotting.\n",
        "    loss_history = []\n",
        "\n",
        "    train_loader = infinite_loader(train_loader)\n",
        "    detector.train()\n",
        "\n",
        "    for _iter in range(max_iters):\n",
        "        # Ignore first arg (image path) during training.\n",
        "        _, images, gt_boxes = next(train_loader)\n",
        "\n",
        "        images = images.to(device)\n",
        "        gt_boxes = gt_boxes.to(device)\n",
        "\n",
        "        # Dictionary of loss scalars.\n",
        "        losses = detector(images, gt_boxes)\n",
        "\n",
        "        # Ignore keys like \"proposals\" in RPN.\n",
        "        losses = {k: v for k, v in losses.items() if \"loss\" in k}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = sum(losses.values())\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Print losses periodically.\n",
        "        if _iter % log_period == 0:\n",
        "            loss_str = f\"[Iter {_iter}][loss: {total_loss:.3f}]\"\n",
        "            for key, value in losses.items():\n",
        "                loss_str += f\"[{key}: {value:.3f}]\"\n",
        "\n",
        "            print(loss_str)\n",
        "            loss_history.append(total_loss.item())\n",
        "\n",
        "    # Plot training loss.\n",
        "    plt.title(\"Training loss history\")\n",
        "    plt.xlabel(f\"Iteration (x {log_period})\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(loss_history)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a wrapper module to contain backbone + RPN:\n",
        "class FirstStage(nn.Module):\n",
        "    def __init__(self, fpn_channels: int):\n",
        "        super().__init__()\n",
        "        self.backbone = DetectorBackboneWithFPN(out_channels=fpn_channels)\n",
        "        self.rpn = RPN(\n",
        "            fpn_channels=fpn_channels,\n",
        "            # Simple stem of two layers:\n",
        "            stem_channels=[fpn_channels, fpn_channels],\n",
        "            batch_size_per_image=16,\n",
        "            anchor_stride_scale=8,\n",
        "            anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
        "            anchor_iou_thresholds=(0.3, 0.6),\n",
        "        )\n",
        "\n",
        "    def forward(self, images, gt_boxes=None):\n",
        "        feats_per_fpn_level = self.backbone(images)\n",
        "        return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n",
        "\n",
        "\n",
        "first_stage = FirstStage(fpn_channels=64).to(DEVICE)\n",
        "\n",
        "train_detector(\n",
        "    first_stage,\n",
        "    small_train_loader,\n",
        "    learning_rate=8e-3,\n",
        "    max_iters=1000,\n",
        "    log_period=20,\n",
        "    device=DEVICE,\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "jKjv6JLMRj7s",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Faster R-CNN\n",
        "\n",
        "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
        "\n",
        "Given a set of proposal boxes from RPN (per FPN level, per image),\n",
        "we warp each region from the correspondng map to a fixed size 7x7 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf).\n",
        "We will use the `roi_align` function from `torchvision`. For usage instructions, see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align\n",
        "\n",
        "For simplicity and computational constraints of Google Colab,\n",
        "our two-stage detector here differs from a standard Faster R-CNN system in the second stage:\n",
        "In a full implementation, the second stage of the network would predict a box deltas to further refine RPN proposals.\n",
        "We omit this for simplicity and keep RPN proposal boxes as final predictions.\n",
        "Your model will definitely perform better if you add an extra box regression head in second stage.\n",
        "\n",
        "### Your implementation exercise\n",
        "\n",
        "Read `FasterRCNN` class documentation and code to understand how everything is pieced together.\n",
        "By now you have already implemented the core components of a typical object detection system - you have dealt with anchor boxes or locations (FCOS), matched them with GT boxes, supervised model with your matching, and wrote inference utilities like NMS.\n",
        "Great work!\n",
        "\n",
        "### Classification Loss: cross entropy\n",
        "\n",
        "The classification loss for second-stage is a cross entropy loss — you would have seen this in A3, and it is a multi-class extension of binary cross entropy loss used in RPN objectness classification. You may use `torch.nn.functional.cross_entropy` directly — follow instructions in Python script.\n",
        "\n",
        "Beyond these, the second stage of Faster R-CNN doesn't add anything that is conceptually new — hence your implementation exercise is fairly lightweight.\n",
        "We have implemented most of this module for you. We left out a few 3-4 line TODO blocks, only because if we wrote them, they would given away the solution for prior exercises (RPN and FCOS).\n",
        "Moreover, empty code blocks will encourage you to carefully read the remaining portions for making everything work.\n",
        "Feel free to refer/re-use your own implementation from the first part of the assignment for filling these blocks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This section requires code changes\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Faster R-CNN detector: this module combines backbone, RPN, ROI predictors.\n",
        "\n",
        "    Unlike Faster R-CNN, we will use class-agnostic box regression and Focal\n",
        "    Loss for classification. We opted for this design choice for you to re-use\n",
        "    a lot of concepts that you already implemented in FCOS - choosing one loss\n",
        "    over other matters less overall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        rpn: nn.Module,\n",
        "        stem_channels: List[int],\n",
        "        num_classes: int,\n",
        "        batch_size_per_image: int,\n",
        "        roi_size: Tuple[int, int] = (7, 7),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.rpn = rpn\n",
        "        self.num_classes = num_classes\n",
        "        self.roi_size = roi_size\n",
        "        self.batch_size_per_image = batch_size_per_image\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Create a stem of alternating 3x3 convolution layers and RELU\n",
        "        # activation modules using `stem_channels` argument, exactly like\n",
        "        # `FCOSPredictionNetwork` and `RPNPredictionNetwork`. use the same\n",
        "        # stride, padding, and weight initialization as previous TODOs.\n",
        "        #\n",
        "        # HINT: This stem will be applied on RoI-aligned FPN features. You can\n",
        "        # decide the number of input channels accordingly.\n",
        "        ######################################################################\n",
        "        # Fill this list. It is okay to use your implementation from\n",
        "        # `FCOSPredictionNetwork` for this code block.\n",
        "        cls_pred = []\n",
        "\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Add an `nn.Flatten` module to `cls_pred`, followed by a linear\n",
        "        # layer to output C+1 classification logits (C classes + background).\n",
        "        # Think about the input size of this linear layer based on the output\n",
        "        # shape from `nn.Flatten` layer.\n",
        "        ######################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        pass\n",
        "        ##########################################################################\n",
        "        #                             END OF YOUR CODE                           #\n",
        "        ##########################################################################\n",
        "        \n",
        "        # Wrap the layers defined by student into a `nn.Sequential` module,\n",
        "        # Faster R-CNN also predicts box offsets to \"refine\" RPN proposals, we\n",
        "        # exclude it for simplicity and keep RPN proposal boxes as final boxes.\n",
        "        self.cls_pred = nn.Sequential(*cls_pred)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: torch.Tensor,\n",
        "        gt_boxes: Optional[torch.Tensor] = None,\n",
        "        test_score_thresh: Optional[float] = None,\n",
        "        test_nms_thresh: Optional[float] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        See documentation of `FCOS.forward` for more details.\n",
        "        \"\"\"\n",
        "\n",
        "        feats_per_fpn_level = self.backbone(images)\n",
        "        output_dict = self.rpn(\n",
        "            feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes\n",
        "        )\n",
        "        proposals_per_fpn_level = output_dict[\"proposals\"]\n",
        "\n",
        "        # Mix GT boxes with proposals. This is necessary to stabilize training\n",
        "        # since RPN proposals may be bad during first few iterations. Also, why\n",
        "        # waste good supervisory signal from GT boxes, for second-stage?\n",
        "        if self.training:\n",
        "            proposals_per_fpn_level = mix_gt_with_proposals(\n",
        "                proposals_per_fpn_level, gt_boxes\n",
        "            )\n",
        "\n",
        "        # Get batch size from FPN feats:\n",
        "        num_images = feats_per_fpn_level[\"p3\"].shape[0]\n",
        "\n",
        "        # Perform RoI-align using FPN features and proposal boxes.\n",
        "        roi_feats_per_fpn_level = {\n",
        "            level_name: None for level_name in feats_per_fpn_level.keys()\n",
        "        }\n",
        "        # Get RPN proposals from all levels.\n",
        "        for level_name in feats_per_fpn_level.keys():\n",
        "            ##################################################################\n",
        "            # TODO: Call `torchvision.ops.roi_align`. See its documentation to\n",
        "            # properly format input arguments. Use `aligned=True`\n",
        "            ##################################################################\n",
        "            level_feats = feats_per_fpn_level[level_name]\n",
        "            level_props = output_dict[\"proposals\"][level_name]\n",
        "            level_stride = self.backbone.fpn_strides[level_name]\n",
        "\n",
        "\n",
        "\n",
        "            # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "            ######################################################################\n",
        "            #                            START YOUR CODE                        #\n",
        "            ######################################################################\n",
        "\n",
        "            pass\n",
        "            ##########################################################################\n",
        "            #                             END OF YOUR CODE                           #\n",
        "            ##################################################################\n",
        "\n",
        "            roi_feats_per_fpn_level[level_name] = roi_feats\n",
        "\n",
        "        # Combine ROI feats across FPN levels, do the same with proposals.\n",
        "        # shape: (batch_size * total_proposals, fpn_channels, roi_h, roi_w)\n",
        "        roi_feats = self._cat_across_fpn_levels(roi_feats_per_fpn_level, dim=0)\n",
        "\n",
        "        # Obtain classification logits for all ROI features.\n",
        "        # shape: (batch_size * total_proposals, num_classes)\n",
        "        pred_cls_logits = self.cls_pred(roi_feats)\n",
        "\n",
        "        if not self.training:\n",
        "            # During inference, just go to this method and skip rest of the\n",
        "            # forward pass. Batch size must be 1!\n",
        "            # fmt: off\n",
        "            return self.inference(\n",
        "                images,\n",
        "                proposals_per_fpn_level,\n",
        "                pred_cls_logits,\n",
        "                test_score_thresh=test_score_thresh,\n",
        "                test_nms_thresh=test_nms_thresh,\n",
        "            )\n",
        "            # fmt: on\n",
        "\n",
        "        ######################################################################\n",
        "        # Match the RPN proposals with provided GT boxes and append to\n",
        "        # `matched_gt_boxes`. Use `rcnn_match_anchors_to_gt` with IoU threshold\n",
        "        # such that IoU > 0.5 is foreground, otherwise background.\n",
        "        # There are no neutral proposals in second-stage.\n",
        "        ######################################################################\n",
        "        matched_gt_boxes = []\n",
        "        for _idx in range(len(gt_boxes)):\n",
        "            # Get proposals per image from this dictionary of list of tensors.\n",
        "            proposals_per_fpn_level_per_image = {\n",
        "                level_name: prop[_idx]\n",
        "                for level_name, prop in output_dict[\"proposals\"].items()\n",
        "            }\n",
        "            proposals_per_image = self._cat_across_fpn_levels(\n",
        "                proposals_per_fpn_level_per_image, dim=0\n",
        "            )\n",
        "            gt_boxes_per_image = gt_boxes[_idx]\n",
        "            # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "            ######################################################################\n",
        "            #                            START YOUR CODE                        #\n",
        "            ######################################################################\n",
        "\n",
        "            pass\n",
        "            ##########################################################################\n",
        "            #                             END OF YOUR CODE                           #\n",
        "            ##########################################################################\n",
        "            \n",
        "        # Combine predictions and GT from across all FPN levels.\n",
        "        matched_gt_boxes = torch.cat(matched_gt_boxes, dim=0)\n",
        "\n",
        "        ######################################################################\n",
        "        # TODO: Train the classifier head. Perform these steps in order:\n",
        "        #   1. Sample a few RPN proposals, like you sampled 50-50% anchor boxes\n",
        "        #      to train RPN objectness classifier. However this time, sample\n",
        "        #      such that ~25% RPN proposals are foreground, and the rest are\n",
        "        #      background. Faster R-CNN performed such weighted sampling to\n",
        "        #      deal with class imbalance, before Focal Loss was published.\n",
        "        #\n",
        "        #   2. Use these indices to get GT class labels from `matched_gt_boxes`\n",
        "        #      and obtain the corresponding logits predicted by classifier.\n",
        "        #\n",
        "        #   3. Compute cross entropy loss - use `F.cross_entropy`, see its API\n",
        "        #      documentation on PyTorch website. Since background ID = -1, you\n",
        "        #      may shift class labels by +1 such that background ID = 0 and\n",
        "        #      other VC classes have IDs (1-20). Make sure to reverse shift\n",
        "        #      this during inference, so that model predicts VOC IDs (0-19).\n",
        "        ######################################################################\n",
        "        # Feel free to delete this line: (but keep variable names same)\n",
        "        loss_cls = None\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        pass\n",
        "        ##########################################################################\n",
        "        #                             END OF YOUR CODE                           #\n",
        "        ##########################################################################\n",
        "\n",
        "        return {\n",
        "            \"loss_rpn_obj\": output_dict[\"loss_rpn_obj\"],\n",
        "            \"loss_rpn_box\": output_dict[\"loss_rpn_box\"],\n",
        "            \"loss_cls\": loss_cls,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_across_fpn_levels(\n",
        "        dict_with_fpn_levels: Dict[str, torch.Tensor], dim: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a dict of tensors across FPN levels {\"p3\", \"p4\", \"p5\"} to a\n",
        "        single tensor. Values could be anything - batches of image features,\n",
        "        GT targets, etc.\n",
        "        \"\"\"\n",
        "        return torch.cat(list(dict_with_fpn_levels.values()), dim=dim)\n",
        "\n",
        "    def inference(\n",
        "        self,\n",
        "        images: torch.Tensor,\n",
        "        proposals: torch.Tensor,\n",
        "        pred_cls_logits: torch.Tensor,\n",
        "        test_score_thresh: float,\n",
        "        test_nms_thresh: float,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run inference on a single input image (batch size = 1). Other input\n",
        "        arguments are same as those computed in `forward` method. This method\n",
        "        should not be called from anywhere except from inside `forward`.\n",
        "\n",
        "        Returns:\n",
        "            Three tensors:\n",
        "                - pred_boxes: Tensor of shape `(N, 4)` giving *absolute* XYXY\n",
        "                  co-ordinates of predicted boxes.\n",
        "\n",
        "                - pred_classes: Tensor of shape `(N, )` giving predicted class\n",
        "                  labels for these boxes (one of `num_classes` labels). Make\n",
        "                  sure there are no background predictions (-1).\n",
        "\n",
        "                - pred_scores: Tensor of shape `(N, )` giving confidence scores\n",
        "                  for predictions.\n",
        "        \"\"\"\n",
        "\n",
        "        # The second stage inference in Faster R-CNN is quite straightforward:\n",
        "        # combine proposals from all FPN levels and perform a *class-specific\n",
        "        # NMS*. There would have been more steps here if we further refined\n",
        "        # RPN proposals by predicting box regression deltas.\n",
        "\n",
        "        # Use `[0]` to remove the batch dimension.\n",
        "        proposals = {level_name: prop[0] for level_name, prop in proposals.items()}\n",
        "        pred_boxes = self._cat_across_fpn_levels(proposals, dim=0)\n",
        "\n",
        "        ######################################################################\n",
        "        # Faster R-CNN inference, perform the following steps in order:\n",
        "        #   1. Get the most confident predicted class and score for every box.\n",
        "        #      Note that the \"score\" of any class (including background) is its\n",
        "        #      probability after applying C+1 softmax.\n",
        "        #\n",
        "        #   2. Only retain prediction that have a confidence score higher than\n",
        "        #      provided threshold in arguments.\n",
        "        #\n",
        "        # NOTE: `pred_classes` may contain background as ID = 0 (based on how\n",
        "        # the classifier was supervised in `forward`). Remember to shift the\n",
        "        # predicted IDs such that model outputs ID (0-19) for 20 VOC classes.\n",
        "        ######################################################################\n",
        "        pred_scores, pred_classes = None, None\n",
        "        # Replace \"pass\" statement with your code\n",
        "        \n",
        "\n",
        "\n",
        "        ######################################################################\n",
        "        #                            START YOUR CODE                        #\n",
        "        ######################################################################\n",
        "\n",
        "        pass\n",
        "        ##########################################################################\n",
        "        #                             END OF YOUR CODE                           #\n",
        "        ##########################################################################\n",
        "\n",
        "\n",
        "        # STUDENTS: This line depends on your implementation of NMS.\n",
        "        keep = class_spec_nms(\n",
        "            pred_boxes, pred_scores, pred_classes, iou_threshold=test_nms_thresh\n",
        "        )\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_classes = pred_classes[keep]\n",
        "        pred_scores = pred_scores[keep]\n",
        "        return pred_boxes, pred_classes, pred_scores\n"
      ],
      "metadata": {
        "id": "m-Z4hlEH73DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "RFZ49wox4MYn",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Overfit small data\n",
        "\n",
        "After adding your implementation, overfit the model on a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "WbxeAJq0zc3F",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Re-initialize dataset objects for independent debugging.\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    train_dataset,\n",
        "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
        ")\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "FPN_CHANNELS = 64\n",
        "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
        "rpn = RPN(\n",
        "    fpn_channels=FPN_CHANNELS,\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=16,\n",
        "    anchor_stride_scale=8,\n",
        "    anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
        "    anchor_iou_thresholds=(0.3, 0.6),\n",
        "    pre_nms_topk=400,\n",
        "    post_nms_topk=80,\n",
        ")\n",
        "\n",
        "# fmt: off\n",
        "faster_rcnn = FasterRCNN(\n",
        "    backbone, rpn, num_classes=20, roi_size=(7, 7),\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=32,\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "train_detector(\n",
        "    faster_rcnn,\n",
        "    small_train_loader,\n",
        "    learning_rate=0.01,\n",
        "    max_iters=1000,\n",
        "    log_period=10,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "_SWA1DbG47ln",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now, follow the instructions in `FasterRCNN.inference` to implement inference, similar to `FCOS.inference`.\n",
        "\n",
        "Visualize the output from the trained model on a few images by executing the next cell, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "gp_Hmt-Km5bl",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Change the loader to have (batch size = 1) as required for inference.\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    small_train_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.2,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "sr7wNngy4oZf",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Train a net\n",
        "\n",
        "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data.\n",
        "We will train for 9000 iterations; this should take about 2-3 hours on a K80 GPU.\n",
        "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
        "\n",
        "(Optional) If you train the model longer (e.g., 25K+ iterations), you should see a better mAP. But make sure you revert the code back for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "X1k1rx1f4sTE",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "reset_seed(0)\n",
        "\n",
        "# Slightly larger detector than in above cell.\n",
        "FPN_CHANNELS = 128\n",
        "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
        "rpn = RPN(\n",
        "    fpn_channels=FPN_CHANNELS,\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=16,\n",
        "    pre_nms_topk=500,\n",
        "    post_nms_topk=200  # Other args from previous cell are default args in RPN.\n",
        ")\n",
        "# fmt: off\n",
        "faster_rcnn = FasterRCNN(\n",
        "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=32,\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "train_detector(\n",
        "    faster_rcnn,\n",
        "    train_loader,\n",
        "    learning_rate=0.01,\n",
        "    max_iters=9000,\n",
        "    log_period=50,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "## Grading : We will look at the loss function figure. The expectation is that loss function decreases continously. We do not expect you to train the \n",
        "## best model possible, significant decrease in loss value which will be seen from the graph is an acceptable solution and will get full marks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "KhWZT-ztEaqm",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### Inference\n",
        "\n",
        "VIsualize a few outputs from the full trained model. They may be less accurate than FCOS.\n",
        "This is expected since our Faster R-CNN model is weaker than expected: we used a smaller model, trained for short duration, and did not include box regression in the second stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "J7ArGiLTnHta",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prepare a small val daataset for inference:\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    val_dataset,\n",
        "    torch.linspace(0, len(val_dataset) - 1, steps=20).long()\n",
        ")\n",
        "\n",
        "small_val_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    small_val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.2,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "ETU6ev7aydIY",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate your Faster R-CNN like FCOS.\n",
        "(**NOTE:** It is okay if your model does not perform very well.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "deletable": true,
        "id": "FvDb7uwqyhAK",
        "jupyter": {
          "outputs_hidden": false
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.2,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "!cd mAP && python main.py\n",
        "\n",
        "\n",
        "# # This script outputs an image containing per-class AP. Display it here:\n",
        "from IPython.display import Image\n",
        "Image(filename=\"./mAP/output/mAP.png\")\n",
        "\n",
        "\n",
        "## For grading - mAP of above 20 per class is fine. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RFZ49wox4MYn"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('homework')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}